{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from numpy import load\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import os,glob\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from math import log\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "### Foolbox imports\n",
    "#from foolbox.attacks import VirtualAdversarialAttack as vaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (0.15.1+cu118)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (3.5.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (1.22.4)\n",
      "Requirement already satisfied: foolbox in /opt/conda/lib/python3.8/site-packages (3.3.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (1.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from foolbox) (1.10.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from foolbox) (68.2.2)\n",
      "Requirement already satisfied: eagerpy>=0.30.0 in /opt/conda/lib/python3.8/site-packages (from foolbox) (0.30.0)\n",
      "Requirement already satisfied: GitPython>=3.0.7 in /opt/conda/lib/python3.8/site-packages (from foolbox) (3.1.43)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from GitPython>=3.0.7->foolbox) (4.0.11)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=3.0.7->foolbox) (5.0.1)\n",
      "\u001b[33mDEPRECATION: torch-tensorrt 1.1.0a0 has a non-standard dependency specifier torch>=1.11.0+cu113<1.12.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torch-tensorrt or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# update python to 3.9 \n",
    "!pip install torch torchvision matplotlib numpy foolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config \n",
    "## load data \n",
    "### set up dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class MalwareDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        # Get list of subdirectories (one for each malware family)\n",
    "        self.families = sorted(os.listdir(root_dir))\n",
    "        self.families.remove('adialer_10.npz')  # Remove this file\n",
    "        # Load images and labels\n",
    "        for i, family in enumerate(self.families):\n",
    "            image_paths = os.listdir(os.path.join(root_dir, family))\n",
    "            for path in image_paths:\n",
    "                self.samples.append(os.path.join(family, path))\n",
    "                self.labels.append(i)\n",
    "    def __len__(self):\n",
    "        return len(self.samples)   \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and label\n",
    "        img_path = os.path.join(self.root_dir, self.samples[idx])\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')    \n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image) \n",
    "        return image, label\n",
    "    \n",
    "# Define data transformations (e.g. normalization, data augmentation)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "filepath = \"/data/saranyav/malimg_gnn/malimg_paper_dataset_imgs/\"\n",
    "# Create train and validation datasets\n",
    "train_dataset = MalwareDataset(filepath, transform=data_transforms)\n",
    "val_dataset = MalwareDataset(filepath, transform=data_transforms)\n",
    "\n",
    "# Create train and validation data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define CNN architecture\n",
    "class MalwareCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MalwareCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x.float())))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize CNN and set device\n",
    "model = MalwareCNN(num_classes=len(train_dataset.families))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "num_classes = len(train_dataset.families)\n",
    "# Initialize CNN and set device\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.0798, Train Acc: 0.9742, Val Loss: 0.0662, Val Acc: 0.9788, Macro Precision: 0.9350, Macro Recall: 0.9326\n",
      "Epoch [2/10], Train Loss: 0.0551, Train Acc: 0.9838, Val Loss: 0.0439, Val Acc: 0.9865, Macro Precision: 0.9497, Macro Recall: 0.9550\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_corrects = 0\n",
    "    total = 0\n",
    "    tp = torch.zeros(num_classes, device=device)\n",
    "    tn = torch.zeros(num_classes, device=device)\n",
    "    fp = torch.zeros(num_classes, device=device)\n",
    "    fn = torch.zeros(num_classes, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            for i in range(num_classes):\n",
    "                tp[i] += ((preds == i) & (labels == i)).sum()\n",
    "                tn[i] += ((preds != i) & (labels != i)).sum()\n",
    "                fp[i] += ((preds == i) & (labels != i)).sum()\n",
    "                fn[i] += ((preds != i) & (labels == i)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = test_corrects.double() / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, (tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_accuracy = running_corrects.double() / len(train_loader.dataset)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy, (tp, tn, fp, fn) = test(model, val_loader, criterion, device, num_classes)\n",
    "\n",
    "    # Optional: Calculate precision and recall for each class\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    \n",
    "    # Handle any NaN values in case of division by zero\n",
    "    precision[torch.isnan(precision)] = 0\n",
    "    recall[torch.isnan(recall)] = 0\n",
    "    \n",
    "    # Calculate macro-averaged precision and recall\n",
    "    macro_precision = precision.mean()\n",
    "    macro_recall = recall.mean()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, '\n",
    "          f'Macro Precision: {macro_precision:.4f}, Macro Recall: {macro_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 0.2160, Train Acc: 0.9324, Val Loss: 0.1001, Val Acc: 0.9740\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Train CNN\n",
    "\n",
    "# num_epochs = 1\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Train for one epoch\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, labels in train_loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "#     # Evaluate on validation set\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_correct = 0\n",
    "#     tp = torch.zeros(num_classes)\n",
    "#     tn = torch.zeros(num_classes)\n",
    "#     fp = torch.zeros(num_classes)\n",
    "#     fn = torch.zeros(num_classes)\n",
    "#     tp = tp.to(device)\n",
    "#     tn = tn.to(device)\n",
    "#     fp = fp.to(device)\n",
    "#     fn = fn.to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)  # move labels to the same device as images\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             val_loss += loss.item() * images.size(0)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             preds = preds.to(device)\n",
    "#             val_correct += torch.sum(preds == labels.data)\n",
    "#             for c in range(num_classes):\n",
    "#                 tp[c] += torch.sum((preds == c) & (labels == c))\n",
    "#                 tn[c] += torch.sum((preds != c) & (labels != c))\n",
    "#                 fp[c] += torch.sum((preds == c) & (labels != c))\n",
    "#                 fn[c] += torch.sum((preds != c) & (labels == c))\n",
    "\n",
    "\n",
    "#     # Print results for epoch\n",
    "#     epoch_loss = running_loss / len(train_dataset)\n",
    "#     epoch_val_loss = val_loss / len(val_dataset)\n",
    "#     epoch_val_acc = val_correct.double() / len(val_dataset)\n",
    "    \n",
    "#     # Compute precision and recall for each class\n",
    "#     precisions = tp / (tp + fp)\n",
    "#     precisions = torch.nan_to_num(precisions)\n",
    "#     recalls = tp / (tp + fn)\n",
    "#     # print(precisions)\n",
    "#     # print(recalls)\n",
    "#     # Compute macro-average precision and recall\n",
    "#     macro_precision = precisions.mean()\n",
    "#     macro_recall = recalls.mean()\n",
    "    \n",
    "#     print('Epoch [{}/{}], Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}, Macro-average Precision: {:.4f}, Macro-average Recall: {:.4f}'\n",
    "#         .format(epoch+1, num_epochs, epoch_loss, epoch_val_loss, epoch_val_acc, macro_precision, macro_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2246, Test Accuracy: 0.9220\n"
     ]
    }
   ],
   "source": [
    "# def test_cnn(model, test_loader, criterion, device):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     test_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * images.size(0)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#     test_loss /= total\n",
    "#     test_accuracy = test_correct / total\n",
    "#     print('Test Loss: {:.4f}, Test Accuracy: {:.4f}'.format(test_loss, test_accuracy))\n",
    "\n",
    "#     return test_loss, test_accuracy\n",
    "\n",
    "# # Test CNN\n",
    "# test_loss, test_accuracy = test_cnn(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 7.56%\n",
      "Validation Loss: 11725.97\n"
     ]
    }
   ],
   "source": [
    "import foolbox as fb\n",
    "import torch\n",
    "\n",
    "fmodel = fb.PyTorchModel(model, bounds=(-1, 1))\n",
    "attack = fb.attacks.L2FastGradientAttack()\n",
    "num_epochs = 1\n",
    "epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    val_correct, val_loss = 0, 0\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        raw_advs, clipped_advs, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "\n",
    "        for adv in clipped_advs:\n",
    "            outputs = fmodel(adv)  # processing each adversarial example tensor individually\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data).item()\n",
    "\n",
    "        val_accuracy = val_correct / len(val_dataset)\n",
    "        print(\"Validation Accuracy: {:.2f}%\".format(100 * val_accuracy))\n",
    "        print(\"Validation Loss: {:.2f}\".format(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 90.52%\n",
      "Validation Loss: 32487.97\n"
     ]
    }
   ],
   "source": [
    "import foolbox as fb\n",
    "import torch\n",
    "\n",
    "fmodel = fb.PyTorchModel(model, bounds=(-1, 1))\n",
    "attack = fb.attacks.L2FastGradientAttack()\n",
    "num_epochs = 1\n",
    "epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0, 3.0, 5.0, 6.0, 10.0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    val_correct, val_loss, total_images = 0, 0, 0\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        _, clipped_advs, _ = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "\n",
    "        # Process each set of adversarial examples corresponding to each epsilon\n",
    "        for adv in clipped_advs:\n",
    "            outputs = fmodel(adv)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)  # multiply by original batch size, not adv.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data).item()\n",
    "            total_images += images.size(0)  # this ensures each epsilon's batch is counted separately\n",
    "\n",
    "    val_accuracy = val_correct / total_images  # Correct calculation of accuracy\n",
    "    print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    print(f\"Validation Loss: {val_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup function for the attack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import foolbox as fb \n",
    "# # preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "# fmodel = fb.PyTorchModel(model, bounds=(-1, 1))\n",
    "\n",
    "# attack = fb.attacks.L2FastGradientAttack()\n",
    "\n",
    "# num_epochs = 1\n",
    "# epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n",
    "\n",
    "# # import numpy as np\n",
    "# # epsilons = np.linspace(0., 50, num=20)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     val_correct,val_loss = 0,0\n",
    "#     # with torch.no_grad():\n",
    "#     for images, labels in val_loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         # print(images.shape,labels.shape)\n",
    "#         # print(\"Checking accuracy: \",foolbox.utils.accuracy(fmodel, images, labels))\n",
    "#         # outputs = fmodel(images)\n",
    "#         # print(images.max(),images.min())\n",
    "#         # raw, clipped, is_adv = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "#         raw_advs, clipped_advs, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "#         # robust_accuracy = 1 - success.float32().mean(axis=-1)\n",
    "#         # print(\"robust accuracy for perturbations with\")\n",
    "#         # for eps, acc in zip(epsilons, robust_accuracy):\n",
    "#         #     print(f\"  Linf norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")\n",
    "\n",
    "#         # print(raw.shape,clipped.shape,is_adv.shape)\n",
    "#         outputs = fmodel(clipped_advs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         val_loss += loss.item() * images.size(0)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         val_correct += torch.sum(preds == labels.data).item()\n",
    "\n",
    "\n",
    "#         # print(torch.mean((fmodel(clipped).argmax(axis=-1) == labels).float()))\n",
    "#     val_correct = val_correct/len(val_dataset)\n",
    "#     print(\"Validation Loss, Validation Correct: \",val_loss,val_correct)\n",
    "\n",
    "#     print(\"Accuracy: \",val_correct)\n",
    "#     print(\"Loss: \",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss, Validation Correct:  1217712.8481063843 0.012421030088874612\n",
      "Macro-average Precision: 0.0005368506305680898\n",
      "Macro-average Recall: 0.04\n",
      "Macro-average F1-score: 0.0010594816759904098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import foolbox\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "#fmodel = foolbox.models.PyTorchModel(model,bounds=(-1,1))\n",
    "preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "fmodel = fb.PyTorchModel(model, bounds=(-1, 1), preprocessing=preprocessing)\n",
    "attack = fb.attacks.L2FastGradientAttack(random_start=True)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "# import numpy as np\n",
    "# epsilons = np.linspace(0., 50, num=20)\n",
    "for epoch in range(num_epochs):\n",
    "    val_correct,val_loss = 0,0\n",
    "    # with torch.no_grad():\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        images = torch.clip(images, -1, 1)\n",
    "        raw, clipped, is_adv = attack(fmodel, images, labels, epsilons=5) #40 \n",
    "        outputs = fmodel(clipped)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        val_correct += torch.sum(preds == labels.data).item()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_correct = val_correct/len(val_dataset)\n",
    "    print(\"Validation Loss, Validation Correct: \",val_loss,val_correct)\n",
    "\n",
    "    # Compute precision, recall, and F1-score for each class\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    # print('Precision:', precision)\n",
    "    # print('Recall:', recall)\n",
    "    # print('F1-score:', fscore)\n",
    "\n",
    "    # Compute macro-average precision\n",
    "    macro_precision = np.mean(precision)\n",
    "    print('Macro-average Precision:', macro_precision)\n",
    "    # Compute macro-average recall\n",
    "    macro_recall = np.mean(recall)\n",
    "    print('Macro-average Recall:', macro_recall)\n",
    "    # Compute macro-average F1-score\n",
    "    macro_fscore = np.mean(fscore)\n",
    "    print('Macro-average F1-score:', macro_fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add robustness via feature squeezing\n",
    "def feature_squeeze(dataset, bit_depth=5):\n",
    "    squeezed_images = []\n",
    "    labels = []\n",
    "    for image, label in dataset:\n",
    "        # Squeeze image\n",
    "        squeezed_image = np.uint8((np.array(image) * (2 ** bit_depth - 1)).round()) / (2 ** bit_depth - 1)\n",
    "        squeezed_images.append(squeezed_image)\n",
    "        labels.append(label)\n",
    "    squeezed_images = np.stack(squeezed_images)\n",
    "    labels = np.array(labels)\n",
    "    print('squeezed_images shape:', squeezed_images.shape)\n",
    "    print('labels shape:', labels.shape)\n",
    "    squeezed_dataset = TensorDataset(torch.from_numpy(squeezed_images), torch.from_numpy(labels))\n",
    "    print('squeezed_dataset type:', type(squeezed_dataset))\n",
    "    return squeezed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squeezed_images shape: (9339, 3, 224, 224)\n",
      "labels shape: (9339,)\n",
      "squeezed_dataset type: <class 'torch.utils.data.dataset.TensorDataset'>\n"
     ]
    }
   ],
   "source": [
    "squeezed_train_dataset = feature_squeeze(train_dataset, bit_depth=4)\n",
    "squeezed_train_loader = DataLoader(squeezed_train_dataset, batch_size=32, shuffle=True)\n",
    "squeezed_val_dataset = feature_squeeze(val_dataset, bit_depth=4)\n",
    "squeezed_val_loader = DataLoader(squeezed_val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 25\n",
    "# Initialize CNN and set device\n",
    "model = MalwareCNN(num_classes=num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Train CNN\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in squeezed_train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    tp = torch.zeros(num_classes)\n",
    "    tn = torch.zeros(num_classes)\n",
    "    fp = torch.zeros(num_classes)\n",
    "    fn = torch.zeros(num_classes)\n",
    "    tp = tp.to(device)\n",
    "    tn = tn.to(device)\n",
    "    fp = fp.to(device)\n",
    "    fn = fn.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in squeezed_val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  # move labels to the same device as images\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            preds = preds.to(device)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "            for c in range(num_classes):\n",
    "                tp[c] += torch.sum((preds == c) & (labels == c))\n",
    "                tn[c] += torch.sum((preds != c) & (labels != c))\n",
    "                fp[c] += torch.sum((preds == c) & (labels != c))\n",
    "                fn[c] += torch.sum((preds != c) & (labels == c))\n",
    "\n",
    "\n",
    "    # Print results for epoch\n",
    "    epoch_loss = running_loss / len(squeezed_train_dataset)\n",
    "    epoch_val_loss = val_loss / len(squeezed_val_dataset)\n",
    "    epoch_val_acc = val_correct.double() / len(squeezed_val_dataset)\n",
    "    \n",
    "    # Compute precision and recall for each class\n",
    "    precisions = tp / (tp + fp)\n",
    "    precisions = torch.nan_to_num(precisions)\n",
    "    # print(precisions)\n",
    "    recalls = tp / (tp + fn)\n",
    "    # print(recalls)\n",
    "\n",
    "    \n",
    "    # Compute macro-average precision and recall\n",
    "    macro_precision = precisions.mean()\n",
    "    macro_recall = recalls.mean()\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}, Macro-average Precision: {:.4f}, Macro-average Recall: {:.4f}'\n",
    "        .format(epoch+1, num_epochs, epoch_loss, epoch_val_loss, epoch_val_acc, macro_precision, macro_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturbn squeezed images using foolbox\n",
    "\n",
    "import foolbox\n",
    "# preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "fmodel = fb.PyTorchModel(model, bounds=(-1, 1), preprocessing=preprocessing)\n",
    "\n",
    "#fmodel = foolbox.models.PyTorchModel(model,bounds=(-1,1))\n",
    "attack = fb.attacks.L2FastGradientAttack(random_start=True)\n",
    "# attack = foolbox.attacks.LinfFastGradientAttack()\n",
    "# Controls whether to randomly start within allowed epsilon ball. If False, starts at original image.\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Set up empty arrays to accumulate predictions and true labels\n",
    "\n",
    "# import numpy as np\n",
    "# epsilons = np.linspace(0., 50, num=20)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    val_correct,val_loss = 0,0\n",
    "    # with torch.no_grad():\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for images, labels in squeezed_val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        images = torch.clip(images, -1, 1)\n",
    "        # print(images.shape,labels.shape)\n",
    "        # print(\"Checking accuracy: \",foolbox.utils.accuracy(fmodel, images, labels))\n",
    "        # outputs = fmodel(images)\n",
    "        # print(images.max(),images.min())\n",
    "        raw, clipped, is_adv = attack(fmodel, images, labels, epsilons=5)\n",
    "        # print(raw.shape,clipped.shape,is_adv.shape)\n",
    "        outputs = fmodel(clipped)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        val_correct += torch.sum(preds == labels.data).item()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # print(torch.mean((fmodel(clipped).argmax(axis=-1) == labels).float()))\n",
    "    val_correct = val_correct/len(squeezed_val_dataset)\n",
    "    # print(\"Validation Loss, Validation Correct: \",val_loss,val_correct)\n",
    "    # Compute precision, recall, and F1-score for each class\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    # print('Precision:', precision)\n",
    "    # print('Recall:', recall)\n",
    "    # print('F1-score:', fscore)\n",
    "\n",
    "    # Compute precision, recall, and F1-score for each class\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    # print('Precision:', precision)\n",
    "    # print('Recall:', recall)\n",
    "    # print('F1-score:', fscore)\n",
    "\n",
    "    # Compute macro-average precision\n",
    "    macro_precision = np.mean(precision)\n",
    "    print('Macro-average Precision:', macro_precision)\n",
    "    # Compute macro-average recall\n",
    "    macro_recall = np.mean(recall)\n",
    "    print('Macro-average Recall:', macro_recall)\n",
    "    # Compute macro-average F1-score\n",
    "    macro_fscore = np.mean(fscore)\n",
    "    print('Macro-average F1-score:', macro_fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "    # Print results for epoch\n",
    "    epoch_val_loss = val_loss / len(val_dataset)\n",
    "    epoch_val_acc = val_correct.double() / len(val_dataset)\n",
    "    \n",
    "    print('Epoch [{}/{}], Val Loss: {:.4f}, Val Acc: {:.4f}'\n",
    "        .format(epoch+1, num_epochs, epoch_val_loss, epoch_val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
