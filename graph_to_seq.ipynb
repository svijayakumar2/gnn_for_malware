{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.cuda import memory    \n",
    "# import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# import pytorch_warmup as warmup  \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # torch.device(\"cuda\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set the fraction of GPU memory to be allocated\n",
    "# memory.set_per_process_memory_fraction(0.5, device=device)\n",
    "# Enable caching allocator to improve memory reuse\n",
    "# memory.set_allocator(memory.PooledMemoryAllocator())\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "# sys.path.append('.')\n",
    "# from utils import change_batchsize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "# import batchnorm\n",
    "from torch_geometric.nn import BatchNorm\n",
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "EPSILON = 0.0# 5 # percentage: ratio of distance of closest centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def change_batchsize(loader, batch_size=16):\n",
    "    torch.manual_seed(1234)\n",
    "    new_loader = DataLoader(loader.dataset, batch_size=batch_size, shuffle = True)\n",
    "    return new_loader\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "batch_size = 64\n",
    "device = 'cuda:0'\n",
    "\n",
    "tr_loader = change_batchsize(\n",
    "    torch.load('imgmalware18_tr_loader.pth'),\n",
    "    batch_size)\n",
    "\n",
    "te_loader = change_batchsize(\n",
    "    torch.load('imgmalware18_te_loader.pth'),\n",
    "    batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# instead of text information from the malware - train feature extractor to get embeddings for it \n",
    "# and then use the embeddings to train the model\n",
    "# for example, LLAVA model: the feature extractor is a vision model\n",
    "# we are working on graph embedding representation of malware. \n",
    "# plug that embedding in, the idea of lava architecture \n",
    "# tune model to do qa from those representations, about malware "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names =  [\"Adialer.C\", \"Allaple.L\", \"C2LOP.gen!g\", \"Dontovo.A\", \"Lolyda.AA1\", \"Lolyda.AT\", \"Rbot!gen\",\n",
    "                \"Swizzor.gen!I\", \"Yuner.A\",\n",
    "                \"Agent.FYI\", \"Alueron.gen!J\", \"C2LOP.P\", \"Fakerean\", \"Lolyda.AA2\", \"Malex.gen!J\", \"Skintrim.N\", \"VB.AT\",\n",
    "                \"Allaple.A\", \"Autorun.K\", \"Dialplatform.B\", \"Instantaccess\", \"Lolyda.AA3\", \"Obfuscator.AD\", \"Swizzor.gen!E\",\n",
    "                \"Wintrim.BX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(10, hidden_channels)\n",
    "        self.batch1 = BatchNorm(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels * 4)\n",
    "        self.batch2 = BatchNorm(hidden_channels * 4)\n",
    "        self.conv3 = GCNConv(hidden_channels * 4, hidden_channels * 8)\n",
    "        self.batch3 = BatchNorm(hidden_channels * 8)\n",
    "        self.conv4 = GCNConv(hidden_channels * 8, hidden_channels)\n",
    "        self.batch4 = BatchNorm(hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.batch5 = BatchNorm(hidden_channels)\n",
    "        self.conv6 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(2 * hidden_channels, hidden_channels)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch1(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch3(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch4(x)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch5(x)\n",
    "        x = self.conv6(x, edge_index)\n",
    "        return x.mean(dim=0)\n",
    "\n",
    "\n",
    "# get embeddings from GCN\n",
    "def get_embeddings(loader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "        embeddings.append(out.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "model = GCN(num_features=10, hidden_channels=64,\n",
    "                    num_classes=24).to(device)\n",
    "\n",
    "# return embeddings\n",
    "tr_embeddings = get_embeddings(tr_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def create_graph_sequence(data, node_order):\n",
    "    # PyTorch Data object with .x and .edge_index\n",
    "    G = nx.Graph()\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        G.add_edge(edge_index[0, i], edge_index[1, i])\n",
    "    # Generate node sequence based on node_order which could be 'centrality' or a traversal like 'dfs'\n",
    "    if node_order == \"centrality\":\n",
    "        centrality_scores = nx.degree_centrality(G)\n",
    "        node_sequence = sorted(centrality_scores, key=centrality_scores.get, reverse=True)\n",
    "    elif node_order == \"dfs\":\n",
    "        node_sequence = list(nx.dfs_preorder_nodes(G, source=0))  # Assuming node 0 as start\n",
    "    # Retrieve embeddings in the order of the node sequence\n",
    "    ordered_embeddings = data.x[node_sequence]\n",
    "    return ordered_embeddings\n",
    "\n",
    "# Apply to all graph data\n",
    "def get_ordered_embeddings(loader, model, order_type):\n",
    "    model.eval()\n",
    "    ordered_embeddings_list = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "        ordered_embeddings = create_graph_sequence(data, order_type)\n",
    "        ordered_embeddings_list.append(ordered_embeddings.cpu().numpy())\n",
    "    return np.vstack(ordered_embeddings_list)\n",
    "\n",
    "# Get ordered embeddings\n",
    "tr_embeddings = get_ordered_embeddings(tr_loader, model, \"centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_embeddings\u001b[39m(embeddings):\n\u001b[1;32m      4\u001b[0m     formatted_embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "def format_embeddings(embeddings):\n",
    "    formatted_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        # Convert each embedding to a string representation\n",
    "        formatted_embedding = \" \".join([str(x) for x in embedding])\n",
    "        formatted_embeddings.append(formatted_embedding)\n",
    "    return formatted_embeddings\n",
    "\n",
    "formatted_tr_embeddings = format_embeddings(tr_embeddings)\n",
    "\n",
    "def predict_with_llm(formatted_embedding):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",  \n",
    "        prompt=f\"Predict the class for the following embedding: {formatted_embedding}\",\n",
    "        max_tokens=10\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "test_embeddings = get_embeddings(te_loader, model)\n",
    "formatted_test_embeddings = format_embeddings(test_embeddings)\n",
    "predictions = [predict_with_llm(fe) for fe in formatted_test_embeddings]\n",
    "\n",
    "# save the predictions in pickle \n",
    "import pickle\n",
    "with open('predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)\n",
    "\n",
    "# save predictions in json\n",
    "import json\n",
    "with open('predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_features = 16  # This should match the output size of the GCN\n",
    "vocab_size = 100  # Adjust according to your actual vocabulary size\n",
    "model = Graph2Seq(embedding_dim=num_features, num_tokens=vocab_size).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the padding index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tr_loader:\n\u001b[1;32m      4\u001b[0m     src \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure this is the embedding output from the GCN\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     trg \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure targets are properly tokenized and shaped\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     src \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension if not present\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     trg_input \u001b[38;5;241m=\u001b[39m trg[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shift trg for input to the Transformer\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/data/data.py:559\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'target'"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for data in tr_loader:\n",
    "        src = data.x.to(device)  # Ensure this is the embedding output from the GCN\n",
    "        trg = data.target.to(device)  # Ensure targets are properly tokenized and shaped\n",
    "\n",
    "        src = src.unsqueeze(0)  # Add batch dimension if not present\n",
    "        trg_input = trg[:-1].unsqueeze(0)  # Shift trg for input to the Transformer\n",
    "        trg_labels = trg[1:].unsqueeze(0)  # Shift trg for output comparison\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg_input)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        trg_labels = trg_labels.view(-1)\n",
    "        loss = criterion(output, trg_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (626466x10 and 16x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         average_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m trg_input \u001b[38;5;241m=\u001b[39m trg[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shift trg for input to the Transformer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m trg_labels \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shift trg for output comparison\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     17\u001b[0m trg_labels \u001b[38;5;241m=\u001b[39m trg_labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mGraph2Seq.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, trg):\n\u001b[0;32m---> 32\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_to_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:, :src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     33\u001b[0m     trg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_to_hidden(trg) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:, :trg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     34\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(src, trg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (626466x10 and 16x16)"
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for data in data_loader:\n",
    "            src = data.x.to(device)  # Assuming the embeddings are stored in x\n",
    "            trg = data.y.to(device)  # Assuming the target sequence indices are in y\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Prepare src and trg for Transformer input\n",
    "            src = src.unsqueeze(1)  # Add a sequence dimension, if necessary\n",
    "            trg_input = trg[:-1].unsqueeze(1)  # Shift trg for input to the Transformer\n",
    "            trg_labels = trg[1:].unsqueeze(1)  # Shift trg for output comparison\n",
    "\n",
    "            output = model(src, trg_input)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            trg_labels = trg_labels.reshape(-1)\n",
    "            loss = criterion(output, trg_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}')\n",
    "\n",
    "train(model, tr_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.0236\n",
      "Epoch 2, Loss: 2.9220\n",
      "Epoch 3, Loss: 2.9028\n",
      "Epoch 4, Loss: 2.9028\n",
      "Epoch 5, Loss: 2.8878\n",
      "Epoch 6, Loss: 2.8854\n",
      "Epoch 7, Loss: 2.8724\n",
      "Epoch 8, Loss: 2.8640\n",
      "Epoch 9, Loss: 2.8394\n",
      "Epoch 10, Loss: 2.8254\n",
      "Epoch 11, Loss: 2.7853\n",
      "Epoch 12, Loss: 2.7351\n",
      "Epoch 13, Loss: 2.6738\n",
      "Epoch 14, Loss: 2.6025\n",
      "Epoch 15, Loss: 2.5713\n",
      "Epoch 16, Loss: 2.5208\n",
      "Epoch 17, Loss: 2.5275\n",
      "Epoch 18, Loss: 2.5339\n",
      "Epoch 19, Loss: 2.4769\n",
      "Epoch 20, Loss: 2.4723\n"
     ]
    }
   ],
   "source": [
    "# # GCN \n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, num_features, num_classes):\n",
    "#         super(GCN, self).__init__()\n",
    "#         self.conv1 = GCNConv(num_features, 16)\n",
    "#         self.conv2 = GCNConv(16, 16)\n",
    "#         self.lin = torch.nn.Linear(16, num_classes)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = global_mean_pool(x, batch)  # Pool node features to get graph-level features\n",
    "\n",
    "#         x = self.lin(x)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# # Initialize the model and optimizer\n",
    "# model = GCN(num_features=10, num_classes=len(label_names)).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# for epoch in range(20):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for data in tr_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data)\n",
    "#         loss = F.nll_loss(out, data.y.to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     average_loss = total_loss / len(tr_loader)\n",
    "#     print(f'Epoch {epoch+1}, Loss: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1889\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in te_loader:\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct += int((pred == data.y.to(device)).sum())\n",
    "    total += data.num_graphs  # todo make sure count the number of graphs, not nodes\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m predict(model, \u001b[43mnew_data\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_names[predicted_label]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in te_loader:\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct += int((pred == data.y.to(device)).sum())\n",
    "    total += data.num_graphs\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        return pred.item()\n",
    "\n",
    "# Example usage\n",
    "new_data = test[0]  # Assuming 'test' is a list of Data objects\n",
    "predicted_label = predict(model, new_data)\n",
    "print(f'Predicted Category: {label_names[predicted_label]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/data/datasets/bodmas/samples/train/'\n",
    "# root_dir = '/data/saranyav/malimg_gnn/exe_files'\n",
    "\n",
    "# # Define dataset class\n",
    "# class MalwareDataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.samples = []\n",
    "#         self.labels = []\n",
    "        \n",
    "#         # Get list of subdirectories (one for each malware family)\n",
    "#         self.families = sorted(os.listdir(root_dir))\n",
    "\n",
    "#         # self.families.remove('adialer_10.npz')  # Remove this file\n",
    "#         # Load graphs and labels\n",
    "#         for i, family in enumerate(self.families):\n",
    "#             # only include graphs that end in exe_cfg \n",
    "#             graph_paths = os.listdir(os.path.join(root_dir, family))\n",
    "#             graph_paths = [path for path in graph_paths if path.endswith('.exe_cfg')]\n",
    "\n",
    "#             for path in graph_paths:\n",
    "#                 self.samples.append(os.path.join(family, path))\n",
    "#                 self.labels.append(i)\n",
    "                \n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Load graph and label\n",
    "#         graph_path = os.path.join(self.root_dir, self.samples[idx])\n",
    "#         label = self.labels[idx]\n",
    "#         graph = torch.LongTensor(np.load(graph_path)['arr_0'])\n",
    "        \n",
    "        \n",
    "#         # # Apply transformations (if any)\n",
    "#         # if self.transform:\n",
    "#         #     image = self.transform(image)\n",
    "        \n",
    "#         return graph, label\n",
    "    \n",
    "# # # Define data transformations (e.g. normalization, data augmentation)\n",
    "# # data_transforms = transforms.Compose([\n",
    "# #     transforms.Resize(256),\n",
    "# #     transforms.CenterCrop(224),\n",
    "# #     transforms.ToTensor(),\n",
    "# #     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "# # ])\n",
    "\n",
    "# malware_dataset = MalwareDataset(root_dir)# , transform=data_transforms)\n",
    "\n",
    "# # Calculate the number of classes\n",
    "# num_classes = len(malware_dataset.families)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adialer.C',\n",
       " 'Agent.FYI',\n",
       " 'Allaple.A',\n",
       " 'Allaple.L',\n",
       " 'Alueron.gen!J',\n",
       " 'Autorun.K',\n",
       " 'C2LOP.P',\n",
       " 'C2LOP.gen!g',\n",
       " 'Dialplatform.B',\n",
       " 'Dontovo.A',\n",
       " 'Fakerean',\n",
       " 'Instantaccess',\n",
       " 'Lolyda.AA1',\n",
       " 'Lolyda.AA2',\n",
       " 'Lolyda.AA3',\n",
       " 'Lolyda.AT',\n",
       " 'Malex.gen!J',\n",
       " 'Obfuscator.AD',\n",
       " 'Rbot!gen',\n",
       " 'Skintrim.N',\n",
       " 'Swizzor.gen!E',\n",
       " 'Swizzor.gen!I',\n",
       " 'VB.AT',\n",
       " 'Wintrim.BX',\n",
       " 'Yuner.A']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# malware_dataset.families "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '/data/saranyav/malimg_gnn/exe_files/Agent.FYI/00f7a755890483dfeb769224ad770800.exe_cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mMalwareDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx])\n\u001b[1;32m     35\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m---> 36\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Apply transformations (if any)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:3008\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m accept_warnings:\n\u001b[1;32m   3007\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m-> 3008\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(\n\u001b[1;32m   3009\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[1;32m   3010\u001b[0m )\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/data/saranyav/malimg_gnn/exe_files/Agent.FYI/00f7a755890483dfeb769224ad770800.exe_cfg'"
     ]
    }
   ],
   "source": [
    "# GNN to encode the CFGs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load the GNN model\n",
    "model = GNN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "# Load the dataset\n",
    "loader = DataLoader(malware_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for data in loader:\n",
    "    optimizer.zero_grad()\n",
    "    data = data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    loss = criterion(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Load the model\n",
    "model = GNN(hidden_channels=64)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for data in loader:\n",
    "    data = data.to(device)\n",
    "    out = model(data.x, data.edge_index, data.batch)\n",
    "    _, predicted = torch.max(out, 1)\n",
    "    total += data.y.size(0)\n",
    "    correct += (predicted == data.y).sum().item()\n",
    "\n",
    "print('Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
