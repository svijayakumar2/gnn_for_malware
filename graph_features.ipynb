{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.cuda import memory    \n",
    "# import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# import pytorch_warmup as warmup  \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # torch.device(\"cuda\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set the fraction of GPU memory to be allocated\n",
    "# memory.set_per_process_memory_fraction(0.5, device=device)\n",
    "# Enable caching allocator to improve memory reuse\n",
    "# memory.set_allocator(memory.PooledMemoryAllocator())\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "# sys.path.append('.')\n",
    "# from utils import change_batchsize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "# import batchnorm\n",
    "from torch_geometric.nn import BatchNorm\n",
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "EPSILON = 0.0# 5 # percentage: ratio of distance of closest centroids\n",
    "import networkx as nx\n",
    "import os\n",
    "import glob\n",
    "# tqdm\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric\n",
    "import statistics\n",
    "# read_dot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 4.54kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 10.5MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 13.3MB/s]\n",
      "Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 209kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [00:03<00:00, 116MB/s]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# It's more efficient to initialize these outside the function if calling multiple times\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def embed_graph(file):\n",
    "    # Load text data from the corresponding _cfg file\n",
    "    try:\n",
    "        with open(file.replace('.npy.npz', '.exe_cfg'), 'r') as f:\n",
    "            text = f.read()\n",
    "    except IOError:\n",
    "        print(f\"Error reading file {file}\")\n",
    "        return None\n",
    "    # Handle the case where the text is empty or very short\n",
    "    if not text:\n",
    "        print(\"No content in file:\", file)\n",
    "        return torch.tensor([])  # Return an empty tensor or some placeholder\n",
    "    # Check and manage text length\n",
    "    encoded_input = tokenizer(text, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    # Use the pooled output for simplicity\n",
    "    node_features = output.pooler_output\n",
    "    return node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in Adialer.C: 122\n",
      "Number of files in Allaple.L: 803\n",
      "Number of files in C2LOP.gen!g: 200\n",
      "Number of files in Dontovo.A: 162\n",
      "Number of files in Lolyda.AA1: 201\n",
      "Number of files in Lolyda.AT: 158\n",
      "Number of files in Rbot!gen: 158\n",
      "Number of files in Swizzor.gen!I: 132\n",
      "Number of files in Yuner.A: 800\n",
      "Number of files in Agent.FYI: 116\n",
      "Number of files in Alueron.gen!J: 198\n",
      "Number of files in C2LOP.P: 146\n",
      "Number of files in Fakerean: 381\n",
      "Number of files in Lolyda.AA2: 184\n",
      "Number of files in Malex.gen!J: 136\n",
      "Number of files in Skintrim.N: 80\n",
      "Number of files in VB.AT: 408\n",
      "Number of files in Allaple.A: 1367\n",
      "Number of files in Autorun.K: 106\n",
      "Number of files in Dialplatform.B: 177\n",
      "Number of files in Instantaccess: 431\n",
      "Number of files in Lolyda.AA3: 13\n",
      "Number of files in Obfuscator.AD: 142\n",
      "Number of files in Swizzor.gen!E: 128\n",
      "Number of files in Wintrim.BX: 97\n"
     ]
    }
   ],
   "source": [
    "labels = [\"Adialer.C\", \"Allaple.L\", \"C2LOP.gen!g\", \"Dontovo.A\", \"Lolyda.AA1\", \"Lolyda.AT\", \"Rbot!gen\",\"Swizzor.gen!I\", \"Yuner.A\",\"Agent.FYI\", \"Alueron.gen!J\", \"C2LOP.P\", \"Fakerean\", \"Lolyda.AA2\", \"Malex.gen!J\", \"Skintrim.N\", \"VB.AT\",\"Allaple.A\", \"Autorun.K\", \"Dialplatform.B\", \"Instantaccess\", \"Lolyda.AA3\", \"Obfuscator.AD\", \"Swizzor.gen!E\",\"Wintrim.BX\"]\n",
    "\n",
    "\n",
    "# dict containing {'graph':,'graph_embed,', 'label', 'input_ids' }\n",
    "d = {}\n",
    "\n",
    "for label in labels:\n",
    "        dir_name = f'/data/saranyav/malimg_gnn/exe_files/{label}/'\n",
    "        # Get a list of files (file paths) in the given directory\n",
    "        # read edge list from file where it is in graph = torch.LongTensor(np.load(‘/data/saranyav/malimg_\n",
    "        # /exe_files/Adialer.C/00a18d2aa4f2f4e6e8b0a8b5f6e2c5a3.npz’)[‘arr_0’])\n",
    "        files = glob.glob(dir_name + '*.npz')\n",
    "        for file in files:\n",
    "                graph = torch.LongTensor(np.load(file)['arr_0'])\n",
    "                graph_embed = []\n",
    "                input_id = file.split('/')[-1].split('.')[0]\n",
    "                d[file] = {'graph': graph, 'graph_embed': graph_embed, 'label': label, 'input_id': input_id}\n",
    "                # print(d[file])\n",
    "                # break\n",
    "\n",
    "# save d to file \n",
    "torch.save(d, '/data/saranyav/llavamal/graphs.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is for generating graph embeddings \n",
    "# for each edgelist in the directory, read the edgelist, embed the graph using the GNN, and save the graph embedding to the dictionary with the graph_embed key\n",
    "\n",
    "def generate_graph_embeddings():\n",
    "    graphs = torch.load('/data/saranyav/llavamal/graphs.pt')\n",
    "    for file in graphs:\n",
    "        graph_embed = embed_graph(file)\n",
    "        # error handle\n",
    "        if graph_embed is None:\n",
    "            continue\n",
    "        graphs[file]['graph_embed'] = graph_embed.cpu().detach().numpy()\n",
    "    torch.save(graphs, '/data/saranyav/llavamal/graphs.pt')\n",
    "\n",
    "generate_graph_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = torch.load('/data/saranyav/llavamal/graphs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '/data/saranyav/malimg_gnn/exe_files/Adialer.C/06e21af7e2fdd67622d0122e113b0d53.exe_cfg'\n",
    "# G = nx.nx_pydot.read_dot(file)\n",
    "# adj_matrix = nx.adjacency_matrix(G)\n",
    "# print(adj_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think this is how i get the edge_lists out?\n",
    "\n",
    "def save_tensor(file):\n",
    "    nxg = nx.nx_pydot.read_dot(file)\n",
    "    print('entered fn')\n",
    "    print('done reading')\n",
    "    for (_, _, d) in nxg.edges(data=True):\n",
    "        print('popping')\n",
    "        d.clear()\n",
    "    G = nx.from_networkx(nxg) # turns it into tensors ['edge_index', 'num_nodes', 'label'] \n",
    "    return(G)\n",
    "\n",
    "\n",
    "overall_stats = [] \n",
    "def save_tensors_malima():\n",
    "    labels = [\"Adialer.C\", \"Allaple.L\", \"C2LOP.gen!g\", \"Dontovo.A\", \"Lolyda.AA1\", \"Lolyda.AT\", \"Rbot!gen\",\n",
    "            \"Swizzor.gen!I\", \"Yuner.A\",\n",
    "            \"Agent.FYI\", \"Alueron.gen!J\", \"C2LOP.P\", \"Fakerean\", \"Lolyda.AA2\", \"Malex.gen!J\", \"Skintrim.N\", \"VB.AT\",\n",
    "            \"Allaple.A\", \"Autorun.K\", \"Dialplatform.B\", \"Instantaccess\", \"Lolyda.AA3\", \"Obfuscator.AD\", \"Swizzor.gen!E\",\n",
    "            \"Wintrim.BX\"]\n",
    "    for label in labels:\n",
    "        dir_name = f'/data/saranyav/malimg_gnn/exe_files/{label}/'\n",
    "        # Get a list of files (file paths) in the given directory\n",
    "        executed_cfg_files = filter(os.path.isfile,\n",
    "                                    glob.glob(dir_name + '*_cfg'))\n",
    "        # Sort list of files in directory by size\n",
    "        executed_cfg_files = sorted(executed_cfg_files,\n",
    "                                    key=lambda x: os.stat(x).st_size)\n",
    "\n",
    "        executed_cfg_files = [x for x in executed_cfg_files if os.stat(x).st_size > 0]\n",
    "        print(f'Number of files in {label}: {len(executed_cfg_files)}')\n",
    "        for file in tqdm(executed_cfg_files, total=len(executed_cfg_files)):\n",
    "            # for file in executed_cfg_files:\n",
    "            try:\n",
    "                G = save_tensor(file)\n",
    "                # print to see if it fails here\n",
    "                print('done')\n",
    "                stats = generate_graphical_features(G)\n",
    "                # save stats, edge_list, graph_ID from filename and label to overall_stats \n",
    "                overall_stats.append({'stats': stats, 'edge_list': G['edge_index'], 'graph_ID': file, 'label': label})\n",
    "            except:\n",
    "                print('no file found ', file)\n",
    "    return(overall_stats)\n",
    "\n",
    "\n",
    "# def save_tensors():\n",
    "\n",
    "#     dir_name = '/data/saranyav/malimg_gnn/exe_files/Adialer.C/'\n",
    "#     # Get a list of files (file paths) in the given directory \n",
    "#     executed_cfg_files = filter( os.path.isfile,\n",
    "#                             glob.glob(dir_name + '*_cfg') )\n",
    "#     # Sort list of files in directory by size \n",
    "#     executed_cfg_files = sorted( executed_cfg_files,\n",
    "#                             key =  lambda x: os.stat(x).st_size )\n",
    "\n",
    "#     executed_cfg_files = [x for x in executed_cfg_files if os.stat(x).st_size > 0]\n",
    "\n",
    "\n",
    "\n",
    "def generate_graphical_features(G):\n",
    "    edges_in = G['edge_index'][0]\n",
    "    edges_out = G['edge_index'][1]\n",
    "\n",
    "    in_deg = torch_geometric.utils.degree(edges_in)\n",
    "    out_deg = torch_geometric.utils.degree(edges_out)\n",
    "\n",
    "    mean_in_deg = statistics.mean(in_deg.tolist())\n",
    "    mean_out_deg = statistics.mean(out_deg.tolist())\n",
    "    median_in_deg = statistics.median(in_deg.tolist())\n",
    "    median_out_deg = statistics.median(out_deg.tolist())\n",
    "    mode_in_deg = statistics.mode(in_deg.tolist())\n",
    "    mode_out_deg = statistics.mode(out_deg.tolist())\n",
    "    nodes = G.num_nodes\n",
    "    edges = G.num_edges\n",
    "\n",
    "    stats = [mean_in_deg, mean_out_deg, median_in_deg, median_out_deg, mode_in_deg, mode_out_deg, nodes, edges] \n",
    "    return(stats)\n",
    "\n",
    "\n",
    "# executed_cfg_files = save_tensors_malima()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: unknown. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Random Forest is a good starting point\u001b[39;00m\n\u001b[1;32m      3\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:390\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of y is not strictly positive which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis necessary for Poisson regression.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m         )\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 390\u001b[0m y, expanded_class_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_y_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m DOUBLE \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m y\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous:\n\u001b[1;32m    393\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(y, dtype\u001b[38;5;241m=\u001b[39mDOUBLE)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:749\u001b[0m, in \u001b[0;36mForestClassifier._validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_y_class_weight\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(y)\n\u001b[1;32m    752\u001b[0m     expanded_class_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:216\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    208\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    215\u001b[0m ]:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: unknown. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def change_batchsize(loader, batch_size=16):\n",
    "    torch.manual_seed(1234)\n",
    "    new_loader = DataLoader(loader.dataset, batch_size=batch_size, shuffle = True)\n",
    "    return new_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "batch_size = 64\n",
    "device = 'cuda:0'\n",
    "\n",
    "tr_loader = change_batchsize(\n",
    "    torch.load('imgmalware18_tr_loader.pth'),\n",
    "    batch_size)\n",
    "\n",
    "te_loader = change_batchsize(\n",
    "    torch.load('imgmalware18_te_loader.pth'),\n",
    "    batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def calculate_graph_features(graph):\n",
    "    features = {}\n",
    "    \n",
    "    # Basic Graph Features\n",
    "    features['number_of_nodes'] = graph.number_of_nodes()\n",
    "    features['number_of_edges'] = graph.number_of_edges()\n",
    "    features['average_degree'] = np.mean([degree for node, degree in graph.degree()])\n",
    "    # median degree, quantiles, degree profile \n",
    "    # Centrality Measures\n",
    "    features['degree_centrality'] = nx.degree_centrality(graph)\n",
    "    features['betweenness_centrality'] = nx.betweenness_centrality(graph)\n",
    "    features['closeness_centrality'] = nx.closeness_centrality(graph)\n",
    "    features['eigenvector_centrality'] = nx.eigenvector_centrality(graph, max_iter=1000)\n",
    "    \n",
    "    # Clustering\n",
    "    features['average_clustering'] = nx.average_clustering(graph)\n",
    "    features['clustering_coefficient'] = nx.clustering(graph)\n",
    "    \n",
    "    # Connectivity\n",
    "    features['is_connected'] = nx.is_connected(graph)\n",
    "    if features['is_connected']:\n",
    "        features['diameter'] = nx.diameter(graph)\n",
    "    else:\n",
    "        features['diameter'] = None\n",
    "    \n",
    "    # Components\n",
    "    features['number_of_connected_components'] = nx.number_connected_components(graph)\n",
    "    features['largest_component_size'] = len(max(nx.connected_components(graph), key=len))\n",
    "    \n",
    "    # Assortativity\n",
    "    features['degree_assortativity_coefficient'] = nx.degree_assortativity_coefficient(graph)\n",
    "    \n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get graph features from each graph in the te_loader \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'number_of_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming te_loader is loaded with your graph data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m graph_te_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgmalware18_te_loader.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m te_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mGraphFeatureDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_te_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m te_loader_with_features \u001b[38;5;241m=\u001b[39m DataLoader(te_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mGraphFeatureDataset.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loader):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraphs \u001b[38;5;241m=\u001b[39m [data \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader]\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_features(graph) \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraphs]\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loader):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraphs \u001b[38;5;241m=\u001b[39m [data \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader]\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraphs]\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mGraphFeatureDataset.calculate_features\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Calculate graph features\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcalculate_graph_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mcalculate_graph_features\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m      5\u001b[0m features \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Basic Graph Features\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_of_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_nodes\u001b[49m()\n\u001b[1;32m      9\u001b[0m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber_of_edges\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mnumber_of_edges()\n\u001b[1;32m     10\u001b[0m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_degree\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([degree \u001b[38;5;28;01mfor\u001b[39;00m node, degree \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mdegree()])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/data/data.py:559\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'number_of_nodes'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming te_loader is loaded with your graph data\n",
    "graph_te_loader = torch.load('imgmalware18_te_loader.pth')\n",
    "te_dataset = GraphFeatureDataset(graph_te_loader)\n",
    "te_loader_with_features = DataLoader(te_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now you can use te_loader_with_features in your model training or evaluation\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "graph_features = calculate_graph_features(G)\n",
    "print(graph_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate graph features\n",
    "## get centrality from graph dataset \n",
    "\n",
    "graphs = torch.load('imgmalware18_graphs.pth')\n",
    "\n",
    "# calculate the centrality of each node in the graphs\n",
    "centrality = []\n",
    "\n",
    "for graph in graphs:\n",
    "    centrality.append(graph.centrality)\n",
    "\n",
    "# get other graph features \n",
    "features = []\n",
    "for graph in graphs:\n",
    "    features.append(graph.x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# entropy \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centrality \n",
    "# triangles \n",
    "# library versus homemade functions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
