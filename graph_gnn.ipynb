{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.cuda import memory    \n",
    "# import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# import pytorch_warmup as warmup  \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # torch.device(\"cuda\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set the fraction of GPU memory to be allocated\n",
    "# memory.set_per_process_memory_fraction(0.5, device=device)\n",
    "# Enable caching allocator to improve memory reuse\n",
    "# memory.set_allocator(memory.PooledMemoryAllocator())\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "# sys.path.append('.')\n",
    "# from utils import change_batchsize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "# import batchnorm\n",
    "from torch_geometric.nn import BatchNorm\n",
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "EPSILON = 0.0# 5 # percentage: ratio of distance of closest centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def change_batchsize(loader, batch_size=16):\n",
    "    torch.manual_seed(1234)\n",
    "    new_loader = DataLoader(loader.dataset, batch_size=batch_size, shuffle = True)\n",
    "    return new_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_load_raw = torch.load('imgmalware18_te_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "batch_size = 64\n",
    "device = 'cuda:0'\n",
    "\n",
    "tr_loader = change_batchsize(\n",
    "    torch.load('imgmalware18_tr_loader.pth'),\n",
    "    batch_size)\n",
    "\n",
    "te_loader = change_batchsize(\n",
    "    torch.load('imgmalware18_te_loader.pth'),\n",
    "    batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN on the graph data \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(3, 64)\n",
    "        self.bn1 = BatchNorm(64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.bn2 = BatchNorm(64)\n",
    "        self.conv3 = GCNConv(64, 64)\n",
    "        self.bn3 = BatchNorm(64)\n",
    "        self.fc1 = Linear(64, 64)\n",
    "        self.fc2 = Linear(64, 1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(10, hidden_channels)\n",
    "        self.batch1 = BatchNorm(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels * 4)\n",
    "        self.batch2 = BatchNorm(hidden_channels * 4)\n",
    "        self.conv3 = GCNConv(hidden_channels * 4, hidden_channels * 8)\n",
    "        self.batch3 = BatchNorm(hidden_channels * 8)\n",
    "        self.conv4 = GCNConv(hidden_channels * 8, hidden_channels)\n",
    "        self.batch4 = BatchNorm(hidden_channels)\n",
    "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.batch5 = BatchNorm(hidden_channels)\n",
    "        self.conv6 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(2 * hidden_channels, hidden_channels)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch1(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch2(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch3(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch4(x)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.batch5(x)\n",
    "        x = self.conv6(x, edge_index)\n",
    "        # 2. Readout layer\n",
    "        x = torch.cat([global_mean_pool(x, batch),\n",
    "                       global_max_pool(x, batch)],\n",
    "                      dim=-1)  # [batch_size, hidden_channels]\n",
    "        # 3. Apply a final classifier\n",
    "        # x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#model = SimpleGCN(num_features=10, hidden_channels=64,\n",
    "#                  num_classes=24).to(device)\n",
    "model = GCN(num_features=10, hidden_channels=64,\n",
    "                    num_classes=24).to(device)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2) \n",
    "# warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=1000)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, Train Loss: 1.8532, Train Acc: 0.3716, Test Loss: 4.8871, Test Acc: 0.2222, Macro Precision: 0.1533, Macro Recall: 0.1667\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "    for data in loader:\n",
    "        data.to(device)\n",
    "        outputs = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(outputs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.item() * data.num_graphs\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == data.y)\n",
    "    train_loss = running_loss / num_examples_in_loader(loader)\n",
    "    train_accuracy = running_corrects.double() / num_examples_in_loader(loader)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def num_examples_in_loader(loader):\n",
    "    total = 0\n",
    "    for data in loader:\n",
    "        total += data.num_graphs\n",
    "    return total\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_corrects = 0\n",
    "    total = 0\n",
    "    tp = torch.zeros(num_classes, device=device)\n",
    "    tn = torch.zeros(num_classes, device=device)\n",
    "    fp = torch.zeros(num_classes, device=device)\n",
    "    fn = torch.zeros(num_classes, device=device)\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data.to(device)\n",
    "            outputs = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            test_loss += loss.item() * data.num_graphs\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_corrects += (preds == data.y).sum()\n",
    "            labels = data.y\n",
    "            for i in range(num_classes):\n",
    "                tp[i] += ((preds == i) & (labels == i)).sum()\n",
    "                tn[i] += ((preds != i) & (labels != i)).sum()\n",
    "                fp[i] += ((preds == i) & (labels != i)).sum()\n",
    "                fn[i] += ((preds != i) & (labels == i)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = test_corrects.double() / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, (tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "num_classes = 24  \n",
    "num_epochs = 1   # Set the desired number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, tr_loader, criterion, optimizer, device)\n",
    "    # Test the model after each epoch\n",
    "    test_loss, test_accuracy, (tp, tn, fp, fn) = test(model, te_loader, criterion, device, num_classes)\n",
    "    # Optional: Calculate precision and recall for each class\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    # Handle any NaN values in case of division by zero\n",
    "    precision[torch.isnan(precision)] = 0\n",
    "    recall[torch.isnan(recall)] = 0\n",
    "    # Calculate macro-averaged precision and recall\n",
    "    macro_precision = precision.mean()\n",
    "    macro_recall = recall.mean()\n",
    "    # Print epoch results\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "          f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}, '\n",
    "          f'Macro Precision: {macro_precision:.4f}, Macro Recall: {macro_recall:.4f}')\n",
    "    # Optional: Update learning rate or perform other updates\n",
    "    optimizer.step()  # If using an optimizer that requires a manual update step\n",
    "    # save epoch results to a file for plotting later\n",
    "    # with open('gnn_normal.csv', 'a') as f:\n",
    "    #     f.write(f'{epoch+1},{train_loss},{train_accuracy},{test_loss},{test_accuracy},{macro_precision},{macro_recall}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def edge_perturbation(data, perturb_rate=0.05):\n",
    "    # Get the number of edges and compute the number of edges to perturb\n",
    "    num_edges = data.edge_index.size(1)\n",
    "    num_perturb = int(num_edges * perturb_rate)\n",
    "    # Assuming the graph is undirected and edge_index is 2 x num_edges\n",
    "    # Removing edges\n",
    "    edge_indices = torch.randperm(num_edges)[:num_perturb]\n",
    "    remaining_edges = torch.ones(num_edges, dtype=torch.bool)\n",
    "    remaining_edges[edge_indices] = False\n",
    "    data.edge_index = data.edge_index[:, remaining_edges]\n",
    "    # Adding edges\n",
    "    num_nodes = data.num_nodes\n",
    "    new_edges = []\n",
    "    for _ in range(num_perturb):\n",
    "        node_a = random.randint(0, num_nodes - 1)\n",
    "        node_b = random.randint(0, num_nodes - 1)\n",
    "        new_edges.append([node_a, node_b])\n",
    "        new_edges.append([node_b, node_a])  # Because it's undirected\n",
    "    new_edges = torch.tensor(new_edges).t().contiguous()\n",
    "    new_edges = new_edges.to(data.edge_index.device)\n",
    "    data.edge_index = torch.cat([data.edge_index, new_edges], dim=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_with_attack(model, loader, criterion, optimizer, device, attack_function, perturb_rate):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for data in loader:\n",
    "        # Apply the attack\n",
    "        attacked_data = attack_function(data.clone(), perturb_rate)  # Clone to avoid modifying original data\n",
    "        attacked_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(attacked_data.x, attacked_data.edge_index, attacked_data.batch)\n",
    "        loss = criterion(outputs, attacked_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * data.num_graphs\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == attacked_data.y)\n",
    "    train_loss = running_loss / num_examples_in_loader(loader)\n",
    "    train_accuracy = running_corrects.double() / num_examples_in_loader(loader)\n",
    "    return train_loss, train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Apply adversarial attack during training\n",
    "    train_loss, train_accuracy = train_epoch_with_attack(model, tr_loader, criterion, optimizer, device, edge_perturbation, 0.05)\n",
    "    # Test the model with clean data\n",
    "    test_loss, test_accuracy, (tp, tn, fp, fn) = test(model, te_loader, criterion, device, num_classes)\n",
    "    # Save results similarly as shown previously\n",
    "    with open('gnn_attacked2.csv', 'a') as f:\n",
    "        f.write(f'{epoch+1},{train_loss},{train_accuracy},{test_loss},{test_accuracy}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 0.67\n",
      "Perturbed Accuracy: 0.22\n",
      "Accuracy Drop: 0.45\n",
      "Cosine Similarity: 0.9882\n",
      "Euclidean Distance: 24.9221\n",
      "Original Accuracy: 0.59\n",
      "Perturbed Accuracy: 0.28\n",
      "Accuracy Drop: 0.31\n",
      "Cosine Similarity: 0.9892\n",
      "Euclidean Distance: 21.1766\n",
      "Original Accuracy: 0.67\n",
      "Perturbed Accuracy: 0.29\n",
      "Accuracy Drop: 0.38\n",
      "Cosine Similarity: 0.9867\n",
      "Euclidean Distance: 26.2492\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# def perturb_features(data, perturb_rate=0.1, device='cuda:0'):\n",
    "#     num_nodes = data.x.size(0)\n",
    "#     num_perturbations = int(num_nodes * perturb_rate)\n",
    "#     perturbation_indices = np.random.choice(num_nodes, num_perturbations, replace=False)\n",
    "#     perturbed_data = data.clone()\n",
    "#     noise = torch.randn(num_perturbations, data.x.size(1), device=device) * 0.1  # Adding Gaussian noise on the correct device\n",
    "#     perturbed_data.x[perturbation_indices] += noise\n",
    "#     return perturbed_data\n",
    "\n",
    "# def perturb_edges(data, perturb_rate=0.05, device='cuda:0'):\n",
    "#     num_edges = data.edge_index.size(1)\n",
    "#     num_perturbations = int(num_edges * perturb_rate)\n",
    "#     perturbed_data = data.clone()\n",
    "#     # Randomly choose edges to remove\n",
    "#     remove_indices = np.random.choice(num_edges, num_perturbations, replace=False)\n",
    "#     mask = torch.ones(num_edges, dtype=torch.bool, device=device)  # Mask generated on the correct device\n",
    "#     mask[remove_indices] = False\n",
    "#     perturbed_data.edge_index = data.edge_index[:, mask]\n",
    "#     # Optionally add random edges\n",
    "#     added_edges = torch.randint(0, data.x.size(0), (2, num_perturbations), dtype=torch.long, device=device)\n",
    "#     perturbed_data.edge_index = torch.cat([perturbed_data.edge_index, added_edges], dim=1)\n",
    "#     return perturbed_data\n",
    "\n",
    "# def adversarial_attack_on_graph(model, data, feature_perturb_rate=0.1, edge_perturb_rate=0.05, device='cuda:0'):\n",
    "#     model.eval()\n",
    "#     data = data.to(device)\n",
    "#     # Apply perturbations\n",
    "#     perturbed_data = perturb_features(data, feature_perturb_rate, device)\n",
    "#     perturbed_data = perturb_edges(perturbed_data, edge_perturb_rate, device)\n",
    "#     with torch.no_grad():\n",
    "#         original_output = model(data.x, data.edge_index, data.batch)\n",
    "#         perturbed_output = model(perturbed_data.x, perturbed_data.edge_index, perturbed_data.batch)\n",
    "#     return original_output, perturbed_output\n",
    "\n",
    "# def calculate_accuracy(logits, labels):\n",
    "#     _, preds = torch.max(logits, 1)\n",
    "#     correct = torch.sum(preds == labels).item()\n",
    "#     return correct / labels.size(0)\n",
    "\n",
    "# def cosine_similarity(a, b):\n",
    "#     return (torch.nn.functional.cosine_similarity(a, b, dim=1).mean()).item()\n",
    "\n",
    "# def euclidean_distance(a, b):\n",
    "#     return torch.norm(a - b, dim=1).mean().item()\n",
    "\n",
    "# def compare_outputs(original_output, perturbed_output, labels):\n",
    "#     # Calculate accuracy for both outputs\n",
    "#     original_accuracy = calculate_accuracy(original_output, labels)\n",
    "#     perturbed_accuracy = calculate_accuracy(perturbed_output, labels)\n",
    "#     accuracy_drop = original_accuracy - perturbed_accuracy\n",
    "#     # Compute output similarity metrics\n",
    "#     similarity_cosine = cosine_similarity(original_output, perturbed_output)\n",
    "#     distance_euclidean = euclidean_distance(original_output, perturbed_output)\n",
    "#     # Print results\n",
    "#     print(f'Original Accuracy: {original_accuracy:.2f}')\n",
    "#     print(f'Perturbed Accuracy: {perturbed_accuracy:.2f}')\n",
    "#     print(f'Accuracy Drop: {accuracy_drop:.2f}')\n",
    "#     print(f'Cosine Similarity: {similarity_cosine:.4f}')\n",
    "#     print(f'Euclidean Distance: {distance_euclidean:.4f}')\n",
    "#     return accuracy_drop, similarity_cosine, distance_euclidean\n",
    "\n",
    "# # Example usage during an evaluation\n",
    "# for data in te_loader:  \n",
    "#     original_output, perturbed_output = adversarial_attack_on_graph(model, data, feature_perturb_rate=0.1, edge_perturb_rate=0.05, device='cuda:0')\n",
    "#     accuracy_drop, similarity_cosine, distance_euclidean = compare_outputs(original_output, perturbed_output, data.y)\n",
    "#     # save results to a file for plotting later\n",
    "#     # with open('adversarial_results.csv', 'a') as f:\n",
    "#     #    f.write(f'{accuracy_drop},{similarity_cosine},{distance_euclidean}\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attack with semantic integrity \n",
    "\n",
    "# def adversarial_attack_on_cfg(model, data, device='cuda:0'):\n",
    "#     model.eval()\n",
    "#     data = data.to(device)\n",
    "    \n",
    "#     perturbed_data = data.clone()\n",
    "#     # Example of Instruction Substitution that maintains semantic integrity\n",
    "#     for node_idx in specific_nodes_to_perturb:\n",
    "#         if perturbed_data.x[node_idx, feature_idx] == original_instruction_code:\n",
    "#             perturbed_data.x[node_idx, feature_idx] = substitute_instruction_code\n",
    "\n",
    "#     # Example of Dead Code Insertion\n",
    "#     # Assuming 'dead_code' is a tensor of no-operation instructions that can be inserted\n",
    "#     perturbed_data.x = torch.cat([perturbed_data.x, dead_code.to(device)], dim=0)\n",
    "#     perturbed_data.edge_index = add_dead_code_edges(perturbed_data.edge_index, device=device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         original_output = model(data.x, data.edge_index, data.batch)\n",
    "#         perturbed_output = model(perturbed_data.x, perturbed_data.edge_index, perturbed_data.batch)\n",
    "\n",
    "#     return original_output, perturbed_output\n",
    "\n",
    "# def add_dead_code_edges(edge_index, num_new_nodes=5, device='cuda:0'):\n",
    "#     # Randomly connect dead code nodes in a way that doesn't alter malware functionality\n",
    "#     new_edges = torch.randint(0, edge_index.max()+1, (2, num_new_nodes), device=device)\n",
    "#     return torch.cat([edge_index, new_edges], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1553,  1.3311, -1.2190,  ..., -1.2033, -0.8135,  0.0518],\n",
      "        [ 1.1692, -2.3956,  1.0116,  ...,  0.1682,  1.3661, -0.4423],\n",
      "        [ 0.0757, -0.8394, -1.1160,  ...,  1.1190, -2.2248, -0.7298],\n",
      "        ...,\n",
      "        [ 0.3633,  0.3458, -0.9276,  ..., -1.6480, -0.4171,  1.1643],\n",
      "        [ 0.0405, -0.6359, -0.5937,  ..., -0.0619, -0.0665,  1.0105],\n",
      "        [ 1.2346,  0.8222,  0.2156,  ...,  0.8017, -0.0228, -0.8778]])\n",
      "tensor([[-1.3988, -1.1265,  0.2347,  ..., -0.6774, -1.2075, -1.0351],\n",
      "        [-0.9664,  1.5618,  0.1210,  ...,  2.2021,  0.6666, -0.4179],\n",
      "        [-0.2692,  0.6058,  0.6797,  ..., -0.4286,  1.7015,  1.1162],\n",
      "        ...,\n",
      "        [ 1.1028,  0.3330,  1.1035,  ...,  0.5084,  0.1475,  0.0186],\n",
      "        [ 0.0947,  1.3832, -0.2032,  ..., -0.0597,  0.6365, -0.3399],\n",
      "        [-0.1870,  0.7057, -1.5771,  ..., -0.0365,  0.0941,  1.3566]])\n",
      "tensor([[-0.6568, -0.2296, -0.3256,  ...,  0.4256, -0.6052, -0.9485],\n",
      "        [ 0.4648, -0.3750, -0.8472,  ..., -1.9158,  0.5079,  0.1575],\n",
      "        [ 0.7629, -2.3081, -0.6200,  ..., -1.2050,  1.0804, -0.3220],\n",
      "        ...,\n",
      "        [ 1.3156, -2.1349,  1.7699,  ..., -0.9733, -0.4823,  0.6590],\n",
      "        [-0.8496,  0.9348,  0.3351,  ...,  0.3245,  1.0632, -0.2027],\n",
      "        [ 1.2113, -1.2632,  1.0538,  ...,  2.6180, -0.0671,  1.2345]])\n"
     ]
    }
   ],
   "source": [
    "# for data in te_loader:\n",
    "#     print(data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Assuming 'data' is a DataBatch from your dataset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m data \u001b[38;5;241m=\u001b[39m tr_loader\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# .dataset[0]\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m perturbed_data \u001b[38;5;241m=\u001b[39m \u001b[43mperturb_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopcode_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_opcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minc_opcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnop_opcode\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mperturb_graph\u001b[0;34m(data, feature_idx, add_opcode, inc_opcode, nop_opcode)\u001b[0m\n\u001b[1;32m     32\u001b[0m perturbed_data\u001b[38;5;241m.\u001b[39medge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([perturbed_data\u001b[38;5;241m.\u001b[39medge_index, dead_code_edges], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Update batch vector to include new nodes\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m last_batch \u001b[38;5;241m=\u001b[39m \u001b[43mperturbed_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     36\u001b[0m dead_code_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m10\u001b[39m,), last_batch, device\u001b[38;5;241m=\u001b[39mperturbed_data\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     37\u001b[0m perturbed_data\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([perturbed_data\u001b[38;5;241m.\u001b[39mbatch, dead_code_batch], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# def get_opcode_for_nop():\n",
    "#     return 0x90  # Opcode for NOP in x86\n",
    "\n",
    "# def get_opcode_for(instruction):\n",
    "#     opcodes = {\n",
    "#         \"ADD\": 0x01,\n",
    "#         \"INC\": 0x40  # Simplified example; actual opcode depends on the operand\n",
    "#     }\n",
    "#     return opcodes[instruction]\n",
    "\n",
    "# def perturb_graph(data, feature_idx, add_opcode, inc_opcode, nop_opcode):\n",
    "#     perturbed_data = data.clone()\n",
    "#     # Assuming we have identified nodes to perturb (e.g., through analysis)\n",
    "#     specific_nodes_to_perturb = [10, 20, 30]  # Placeholder\n",
    "#     # Instruction substitution\n",
    "#     for node_idx in specific_nodes_to_perturb:\n",
    "#         if perturbed_data.x[node_idx, feature_idx] == add_opcode:\n",
    "#             perturbed_data.x[node_idx, feature_idx] = inc_opcode\n",
    "#     # Adding dead code: let's add 10 NOP nodes\n",
    "#     num_nodes = perturbed_data.x.size(0)\n",
    "#     dead_code = torch.full((10, perturbed_data.x.size(1)), nop_opcode, device=perturbed_data.x.device)\n",
    "#     perturbed_data.x = torch.cat([perturbed_data.x, dead_code], dim=0)\n",
    "#     # Adjust edge_index by appending isolated edges or no edges for dead code\n",
    "#     # Here, we add no edges for the dead code, making them isolated nodes\n",
    "#     dead_code_edges = torch.tensor([], dtype=torch.long, device=data.edge_index.device)\n",
    "#     perturbed_data.edge_index = torch.cat([perturbed_data.edge_index, dead_code_edges], dim=1)\n",
    "#     # Update batch vector to include new nodes\n",
    "#     last_batch = perturbed_data.batch[-1]\n",
    "#     dead_code_batch = torch.full((10,), last_batch, device=perturbed_data.batch.device)\n",
    "#     perturbed_data.batch = torch.cat([perturbed_data.batch, dead_code_batch], dim=0)\n",
    "#     return perturbed_data\n",
    "\n",
    "# # Example use\n",
    "# opcode_idx = 5  # Feature index for opcode\n",
    "# add_opcode = get_opcode_for(\"ADD\")\n",
    "# inc_opcode = get_opcode_for(\"INC\")\n",
    "# nop_opcode = get_opcode_for_nop()\n",
    "\n",
    "# # Assuming 'data' is a DataBatch from your dataset\n",
    "# data = tr_loader.dataset[0] # .dataset[0]\n",
    "# perturbed_data = perturb_graph(data, opcode_idx, add_opcode, inc_opcode, nop_opcode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch_geometric.nn import GCNConv, BatchNorm, global_mean_pool, global_max_pool\n",
    "# from torch_geometric.nn import Linear\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, num_features, hidden_channels, num_classes):\n",
    "#         super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "#         self.batch1 = BatchNorm(hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels * 4)\n",
    "#         self.batch2 = BatchNorm(hidden_channels * 4)\n",
    "#         self.conv3 = GCNConv(hidden_channels * 4, hidden_channels * 8)\n",
    "#         self.batch3 = BatchNorm(hidden_channels * 8)\n",
    "#         self.conv4 = GCNConv(hidden_channels * 8, hidden_channels)\n",
    "#         self.batch4 = BatchNorm(hidden_channels)\n",
    "#         self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.batch5 = BatchNorm(hidden_channels)\n",
    "#         self.conv6 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.lin = Linear(2 * hidden_channels, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.batch1(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.batch2(x)\n",
    "#         x = self.conv3(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.batch3(x)\n",
    "#         x = self.conv4(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.batch4(x)\n",
    "#         x = self.conv5(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.batch5(x)\n",
    "#         x = self.conv6(x, edge_index)\n",
    "#         x = torch.cat([global_mean_pool(x, batch), global_max_pool(x, batch)], dim=-1)\n",
    "#         x = self.lin(x)\n",
    "#         return x\n",
    "\n",
    "# # Define the model\n",
    "# model = GCN(num_features=10, hidden_channels=64, num_classes=24).to(device)\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'edge_index' and 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m test(tr_loader)\n\u001b[1;32m      6\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(te_loader)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'edge_index' and 'batch'"
     ]
    }
   ],
   "source": [
    "\n",
    "# def test(*, model, loader, device='cuda:0', num_classes):\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_correct = 0\n",
    "#     tp = torch.zeros(num_classes)\n",
    "#     tn = torch.zeros(num_classes)\n",
    "#     fp = torch.zeros(num_classes)\n",
    "#     fn = torch.zeros(num_classes)\n",
    "#     tp = tp.to(device)\n",
    "#     tn = tn.to(device)\n",
    "#     fp = fp.to(device)\n",
    "#     fn = fn.to(device)\n",
    "#     num_examples = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data in loader:\n",
    "#             num_examples += int(data.num_graphs)\n",
    "#             data.to(device)\n",
    "#             outputs = model(data.x, data.edge_index, data.batch)\n",
    "#             loss = criterion(outputs, data.y)\n",
    "#             val_loss += loss.item() * int(data.num_graphs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             preds = preds.to(device)\n",
    "#             val_correct += torch.sum(preds == data.y)\n",
    "#             for c in range(num_classes):\n",
    "#                 tp[c] += torch.sum((preds == c) & (data.y == c))\n",
    "#                 tn[c] += torch.sum((preds != c) & (data.y != c))\n",
    "#                 fp[c] += torch.sum((preds == c) & (data.y != c))\n",
    "#                 fn[c] += torch.sum((preds != c) & (data.y == c))\n",
    "\n",
    "#     epoch_val_loss = val_loss / num_examples\n",
    "#     epoch_val_acc = val_correct.double() / num_examples\n",
    "#     precisions = tp / (tp + fp)\n",
    "#     recalls = tp / (tp + fn)\n",
    "#     macro_precision = precisions.mean()\n",
    "#     macro_recall = recalls.mean()\n",
    "\n",
    "#     print('Test Loss: {:.4f}, Test Acc: {:.4f}, Macro-average Precision: {:.4f}, Macro-average Recall: {:.4f}'.format(epoch_val_loss, epoch_val_acc, macro_precision, macro_recall))\n",
    "\n",
    "#     return epoch_val_loss, epoch_val_acc, macro_precision, macro_recall\n",
    "\n",
    "# # Create lists to store results\n",
    "# train_losses = []\n",
    "# train_acc_no_rejects = []\n",
    "# train_acc_with_rejects = []\n",
    "# test_losses = []\n",
    "# test_acc_no_rejects = []\n",
    "# test_acc_with_rejects = []\n",
    "\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# # Define your optimizer and learning rate parameters\n",
    "# initial_lr = 0.01  # Adjust the initial learning rate as needed\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)#, weight_decay=5e-4)\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, initial_lr, 0.0001, 5000)\n",
    "\n",
    "# # MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.1)\n",
    "\n",
    "# # warmup_epochs = 10  # Number of epochs for warm-up\n",
    "# total_epochs = 5000  # Total number of training epochs\n",
    "# # lr_lambda = lambda epoch: min(1.0, (epoch + 1) / warmup_epochs)  # Linear warm-up function\n",
    "\n",
    "# # Create the scheduler, monitoring validation loss\n",
    "# # lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "# # Define the learning rate scheduler\n",
    "# # scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(total_epochs):\n",
    "#     scheduler.step()  # Adjust the learning rate\n",
    "#     loss, train_acc_no_reject, train_acc_with_reject = train_epoch(model=model,\n",
    "#                                     loader=tr_loader,\n",
    "#                                     criterion=criterion,\n",
    "#                                     optimizer=optimizer,\n",
    "#                                     device=device)\n",
    "#     test_loss, test_acc_no_reject, test_acc_with_reject = test(model=model, loader=te_loader, device=device)\n",
    "    \n",
    "#     print(\n",
    "#         f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train Acc No Reject: {train_acc_no_reject:.4f}, Train Acc With Reject: {test_acc_with_reject:.4f}, Test Loss: {test_loss:.4f}, Test Acc No Reject: {test_acc_no_reject:.4f} , Test Acc With Reject: {test_acc_with_reject:.4f}'\n",
    "#     )\n",
    "#     #print(optimizer.param_groups[0]['lr'])  # Print the current learning rate\n",
    "#     # Append results to lists\n",
    "#     train_losses.append(loss)\n",
    "#     train_acc_no_rejects.append(train_acc_no_reject)\n",
    "#     train_acc_with_rejects.append(train_acc_with_reject)\n",
    "#     test_losses.append(test_loss)\n",
    "#     test_acc_no_rejects.append(test_acc_no_reject)\n",
    "#     test_acc_with_rejects.append(test_acc_with_reject)\n",
    "#     #lr_scheduler.step(test_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # graph attack to test the robustness of the model\n",
    "\n",
    "# def attack(data, model, criterion, epsilon):\n",
    "#     model.eval()\n",
    "#     data = data.to(device)\n",
    "#     output = model(data)\n",
    "#     loss = criterion(output, data.y.view(-1, 1).float())\n",
    "#     loss.backward()\n",
    "#     grad = data.x.grad\n",
    "#     data.x = data.x + epsilon * grad.sign()\n",
    "#     data.x = data.x.clamp(-1, 1)\n",
    "#     return data\n",
    "\n",
    "# def attack_test(loader, model, criterion, epsilon):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     for data in loader:\n",
    "#         data = attack(data, model, criterion, epsilon)\n",
    "#         data = data.to(device)\n",
    "#         pred = model(data).view(-1)\n",
    "#         pred = (pred > 0).float()\n",
    "#         correct += pred.eq(data.y).sum().item()\n",
    "#     return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 0.0601, Train Acc No Reject: 0.1395, Train Acc With Reject: 0.0000, Test Loss: 0.0893, Test Acc No Reject: 0.0000 , Test Acc With Reject: 0.0000\n",
      "Epoch: 001, Train Loss: 0.0455, Train Acc No Reject: 0.3179, Train Acc With Reject: 0.0556, Test Loss: 0.0920, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 002, Train Loss: 0.0376, Train Acc No Reject: 0.3975, Train Acc With Reject: 0.0389, Test Loss: 0.1350, Test Acc No Reject: 0.0389 , Test Acc With Reject: 0.0389\n",
      "Epoch: 003, Train Loss: 0.0327, Train Acc No Reject: 0.4765, Train Acc With Reject: 0.0056, Test Loss: 0.1900, Test Acc No Reject: 0.0056 , Test Acc With Reject: 0.0056\n",
      "Epoch: 004, Train Loss: 0.0286, Train Acc No Reject: 0.5846, Train Acc With Reject: 0.0556, Test Loss: 0.1251, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 005, Train Loss: 0.0257, Train Acc No Reject: 0.6370, Train Acc With Reject: 0.0056, Test Loss: 0.2113, Test Acc No Reject: 0.0056 , Test Acc With Reject: 0.0056\n",
      "Epoch: 006, Train Loss: 0.0234, Train Acc No Reject: 0.6747, Train Acc With Reject: 0.0111, Test Loss: 0.1935, Test Acc No Reject: 0.0111 , Test Acc With Reject: 0.0111\n",
      "Epoch: 007, Train Loss: 0.0210, Train Acc No Reject: 0.6920, Train Acc With Reject: 0.0556, Test Loss: 0.4036, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 008, Train Loss: 0.0188, Train Acc No Reject: 0.6988, Train Acc With Reject: 0.0389, Test Loss: 0.2846, Test Acc No Reject: 0.0389 , Test Acc With Reject: 0.0389\n",
      "Epoch: 009, Train Loss: 0.0174, Train Acc No Reject: 0.7012, Train Acc With Reject: 0.0556, Test Loss: 0.7133, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 010, Train Loss: 0.0163, Train Acc No Reject: 0.7086, Train Acc With Reject: 0.0167, Test Loss: 0.3046, Test Acc No Reject: 0.0167 , Test Acc With Reject: 0.0167\n",
      "Epoch: 011, Train Loss: 0.0151, Train Acc No Reject: 0.7259, Train Acc With Reject: 0.0611, Test Loss: 0.1992, Test Acc No Reject: 0.0611 , Test Acc With Reject: 0.0611\n",
      "Epoch: 012, Train Loss: 0.0141, Train Acc No Reject: 0.7309, Train Acc With Reject: 0.0611, Test Loss: 1.2736, Test Acc No Reject: 0.0611 , Test Acc With Reject: 0.0611\n",
      "Epoch: 013, Train Loss: 0.0141, Train Acc No Reject: 0.7340, Train Acc With Reject: 0.0556, Test Loss: 1.2473, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 014, Train Loss: 0.0131, Train Acc No Reject: 0.7469, Train Acc With Reject: 0.0167, Test Loss: 0.1870, Test Acc No Reject: 0.0167 , Test Acc With Reject: 0.0167\n",
      "Epoch: 015, Train Loss: 0.0121, Train Acc No Reject: 0.7525, Train Acc With Reject: 0.0500, Test Loss: 0.5988, Test Acc No Reject: 0.0500 , Test Acc With Reject: 0.0500\n",
      "Epoch: 016, Train Loss: 0.0119, Train Acc No Reject: 0.7420, Train Acc With Reject: 0.0556, Test Loss: 0.4724, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 017, Train Loss: 0.0110, Train Acc No Reject: 0.7494, Train Acc With Reject: 0.0556, Test Loss: 0.4832, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 018, Train Loss: 0.0108, Train Acc No Reject: 0.7420, Train Acc With Reject: 0.1000, Test Loss: 0.8094, Test Acc No Reject: 0.1000 , Test Acc With Reject: 0.1000\n",
      "Epoch: 019, Train Loss: 0.0104, Train Acc No Reject: 0.7623, Train Acc With Reject: 0.0500, Test Loss: 0.7698, Test Acc No Reject: 0.0500 , Test Acc With Reject: 0.0500\n",
      "Epoch: 020, Train Loss: 0.0101, Train Acc No Reject: 0.7537, Train Acc With Reject: 0.0556, Test Loss: 0.5508, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 021, Train Loss: 0.0097, Train Acc No Reject: 0.7593, Train Acc With Reject: 0.0389, Test Loss: 0.3479, Test Acc No Reject: 0.0389 , Test Acc With Reject: 0.0389\n",
      "Epoch: 022, Train Loss: 0.0094, Train Acc No Reject: 0.7735, Train Acc With Reject: 0.0556, Test Loss: 0.5043, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 023, Train Loss: 0.0091, Train Acc No Reject: 0.7617, Train Acc With Reject: 0.1611, Test Loss: 0.2663, Test Acc No Reject: 0.1611 , Test Acc With Reject: 0.1611\n",
      "Epoch: 024, Train Loss: 0.0090, Train Acc No Reject: 0.7735, Train Acc With Reject: 0.0000, Test Loss: 0.4129, Test Acc No Reject: 0.0000 , Test Acc With Reject: 0.0000\n",
      "Epoch: 025, Train Loss: 0.0087, Train Acc No Reject: 0.7747, Train Acc With Reject: 0.0556, Test Loss: 0.5778, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 026, Train Loss: 0.0087, Train Acc No Reject: 0.7889, Train Acc With Reject: 0.1000, Test Loss: 0.2664, Test Acc No Reject: 0.1000 , Test Acc With Reject: 0.1000\n",
      "Epoch: 027, Train Loss: 0.0088, Train Acc No Reject: 0.7852, Train Acc With Reject: 0.0556, Test Loss: 0.4501, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 028, Train Loss: 0.0082, Train Acc No Reject: 0.7802, Train Acc With Reject: 0.0667, Test Loss: 0.5282, Test Acc No Reject: 0.0667 , Test Acc With Reject: 0.0667\n",
      "Epoch: 029, Train Loss: 0.0079, Train Acc No Reject: 0.7920, Train Acc With Reject: 0.0056, Test Loss: 0.6939, Test Acc No Reject: 0.0056 , Test Acc With Reject: 0.0056\n",
      "Epoch: 030, Train Loss: 0.0082, Train Acc No Reject: 0.7778, Train Acc With Reject: 0.0556, Test Loss: 0.4635, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 031, Train Loss: 0.0082, Train Acc No Reject: 0.7827, Train Acc With Reject: 0.0556, Test Loss: 0.7097, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 032, Train Loss: 0.0078, Train Acc No Reject: 0.7957, Train Acc With Reject: 0.0889, Test Loss: 0.3322, Test Acc No Reject: 0.0889 , Test Acc With Reject: 0.0889\n",
      "Epoch: 033, Train Loss: 0.0077, Train Acc No Reject: 0.7944, Train Acc With Reject: 0.0556, Test Loss: 0.8395, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 034, Train Loss: 0.0075, Train Acc No Reject: 0.7938, Train Acc With Reject: 0.1778, Test Loss: 0.2994, Test Acc No Reject: 0.1778 , Test Acc With Reject: 0.1778\n",
      "Epoch: 035, Train Loss: 0.0074, Train Acc No Reject: 0.8049, Train Acc With Reject: 0.0556, Test Loss: 0.7056, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 036, Train Loss: 0.0073, Train Acc No Reject: 0.8043, Train Acc With Reject: 0.0556, Test Loss: 0.7837, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 037, Train Loss: 0.0075, Train Acc No Reject: 0.7975, Train Acc With Reject: 0.1111, Test Loss: 0.2676, Test Acc No Reject: 0.1111 , Test Acc With Reject: 0.1111\n",
      "Epoch: 038, Train Loss: 0.0071, Train Acc No Reject: 0.8000, Train Acc With Reject: 0.1333, Test Loss: 0.2299, Test Acc No Reject: 0.1333 , Test Acc With Reject: 0.1333\n",
      "Epoch: 039, Train Loss: 0.0071, Train Acc No Reject: 0.7969, Train Acc With Reject: 0.1444, Test Loss: 0.3383, Test Acc No Reject: 0.1444 , Test Acc With Reject: 0.1444\n",
      "Epoch: 040, Train Loss: 0.0070, Train Acc No Reject: 0.8056, Train Acc With Reject: 0.0667, Test Loss: 0.3526, Test Acc No Reject: 0.0667 , Test Acc With Reject: 0.0667\n",
      "Epoch: 041, Train Loss: 0.0073, Train Acc No Reject: 0.7969, Train Acc With Reject: 0.1944, Test Loss: 0.1155, Test Acc No Reject: 0.1944 , Test Acc With Reject: 0.1944\n",
      "Epoch: 042, Train Loss: 0.0073, Train Acc No Reject: 0.7833, Train Acc With Reject: 0.0889, Test Loss: 0.3323, Test Acc No Reject: 0.0889 , Test Acc With Reject: 0.0889\n",
      "Epoch: 043, Train Loss: 0.0069, Train Acc No Reject: 0.8148, Train Acc With Reject: 0.0556, Test Loss: 0.7963, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 044, Train Loss: 0.0067, Train Acc No Reject: 0.8160, Train Acc With Reject: 0.1000, Test Loss: 0.6270, Test Acc No Reject: 0.1000 , Test Acc With Reject: 0.1000\n",
      "Epoch: 045, Train Loss: 0.0069, Train Acc No Reject: 0.8173, Train Acc With Reject: 0.0389, Test Loss: 0.4079, Test Acc No Reject: 0.0389 , Test Acc With Reject: 0.0389\n",
      "Epoch: 046, Train Loss: 0.0068, Train Acc No Reject: 0.8074, Train Acc With Reject: 0.0556, Test Loss: 0.5130, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 047, Train Loss: 0.0068, Train Acc No Reject: 0.8105, Train Acc With Reject: 0.1667, Test Loss: 0.2854, Test Acc No Reject: 0.1667 , Test Acc With Reject: 0.1667\n",
      "Epoch: 048, Train Loss: 0.0065, Train Acc No Reject: 0.8278, Train Acc With Reject: 0.0667, Test Loss: 0.2889, Test Acc No Reject: 0.0667 , Test Acc With Reject: 0.0667\n",
      "Epoch: 049, Train Loss: 0.0066, Train Acc No Reject: 0.8148, Train Acc With Reject: 0.0778, Test Loss: 0.5668, Test Acc No Reject: 0.0778 , Test Acc With Reject: 0.0778\n",
      "Epoch: 050, Train Loss: 0.0064, Train Acc No Reject: 0.8160, Train Acc With Reject: 0.0556, Test Loss: 0.8197, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 051, Train Loss: 0.0063, Train Acc No Reject: 0.8216, Train Acc With Reject: 0.0556, Test Loss: 1.5630, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 052, Train Loss: 0.0064, Train Acc No Reject: 0.8198, Train Acc With Reject: 0.1111, Test Loss: 0.5317, Test Acc No Reject: 0.1111 , Test Acc With Reject: 0.1111\n",
      "Epoch: 053, Train Loss: 0.0065, Train Acc No Reject: 0.8148, Train Acc With Reject: 0.0556, Test Loss: 0.2721, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 054, Train Loss: 0.0064, Train Acc No Reject: 0.8191, Train Acc With Reject: 0.0556, Test Loss: 0.8943, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 055, Train Loss: 0.0062, Train Acc No Reject: 0.8247, Train Acc With Reject: 0.0556, Test Loss: 0.7093, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 056, Train Loss: 0.0061, Train Acc No Reject: 0.8296, Train Acc With Reject: 0.1111, Test Loss: 0.1618, Test Acc No Reject: 0.1111 , Test Acc With Reject: 0.1111\n",
      "Epoch: 057, Train Loss: 0.0062, Train Acc No Reject: 0.8321, Train Acc With Reject: 0.1444, Test Loss: 0.1990, Test Acc No Reject: 0.1444 , Test Acc With Reject: 0.1444\n",
      "Epoch: 058, Train Loss: 0.0063, Train Acc No Reject: 0.8160, Train Acc With Reject: 0.0444, Test Loss: 0.2320, Test Acc No Reject: 0.0444 , Test Acc With Reject: 0.0444\n",
      "Epoch: 059, Train Loss: 0.0061, Train Acc No Reject: 0.8198, Train Acc With Reject: 0.0000, Test Loss: 0.5779, Test Acc No Reject: 0.0000 , Test Acc With Reject: 0.0000\n",
      "Epoch: 060, Train Loss: 0.0064, Train Acc No Reject: 0.8247, Train Acc With Reject: 0.0944, Test Loss: 0.3197, Test Acc No Reject: 0.0944 , Test Acc With Reject: 0.0944\n",
      "Epoch: 061, Train Loss: 0.0063, Train Acc No Reject: 0.8235, Train Acc With Reject: 0.0611, Test Loss: 0.3192, Test Acc No Reject: 0.0611 , Test Acc With Reject: 0.0611\n",
      "Epoch: 062, Train Loss: 0.0060, Train Acc No Reject: 0.8327, Train Acc With Reject: 0.1444, Test Loss: 0.2452, Test Acc No Reject: 0.1444 , Test Acc With Reject: 0.1444\n",
      "Epoch: 063, Train Loss: 0.0060, Train Acc No Reject: 0.8321, Train Acc With Reject: 0.0556, Test Loss: 1.5646, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 064, Train Loss: 0.0061, Train Acc No Reject: 0.8216, Train Acc With Reject: 0.0722, Test Loss: 0.5132, Test Acc No Reject: 0.0722 , Test Acc With Reject: 0.0722\n",
      "Epoch: 065, Train Loss: 0.0060, Train Acc No Reject: 0.8228, Train Acc With Reject: 0.0556, Test Loss: 2.3504, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 066, Train Loss: 0.0061, Train Acc No Reject: 0.8370, Train Acc With Reject: 0.1444, Test Loss: 0.2315, Test Acc No Reject: 0.1444 , Test Acc With Reject: 0.1444\n",
      "Epoch: 067, Train Loss: 0.0058, Train Acc No Reject: 0.8401, Train Acc With Reject: 0.1111, Test Loss: 0.3782, Test Acc No Reject: 0.1111 , Test Acc With Reject: 0.1111\n",
      "Epoch: 068, Train Loss: 0.0060, Train Acc No Reject: 0.8438, Train Acc With Reject: 0.0556, Test Loss: 0.3823, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 069, Train Loss: 0.0060, Train Acc No Reject: 0.8358, Train Acc With Reject: 0.0556, Test Loss: 1.1524, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 070, Train Loss: 0.0061, Train Acc No Reject: 0.8278, Train Acc With Reject: 0.0667, Test Loss: 0.4981, Test Acc No Reject: 0.0667 , Test Acc With Reject: 0.0667\n",
      "Epoch: 071, Train Loss: 0.0056, Train Acc No Reject: 0.8228, Train Acc With Reject: 0.0444, Test Loss: 2.7884, Test Acc No Reject: 0.0444 , Test Acc With Reject: 0.0444\n",
      "Epoch: 072, Train Loss: 0.0056, Train Acc No Reject: 0.8395, Train Acc With Reject: 0.1000, Test Loss: 1.3468, Test Acc No Reject: 0.1000 , Test Acc With Reject: 0.1000\n",
      "Epoch: 073, Train Loss: 0.0057, Train Acc No Reject: 0.8451, Train Acc With Reject: 0.1222, Test Loss: 0.5350, Test Acc No Reject: 0.1222 , Test Acc With Reject: 0.1222\n",
      "Epoch: 074, Train Loss: 0.0056, Train Acc No Reject: 0.8444, Train Acc With Reject: 0.0556, Test Loss: 0.5953, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 075, Train Loss: 0.0056, Train Acc No Reject: 0.8451, Train Acc With Reject: 0.0556, Test Loss: 0.4305, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 076, Train Loss: 0.0057, Train Acc No Reject: 0.8377, Train Acc With Reject: 0.0889, Test Loss: 0.3509, Test Acc No Reject: 0.0889 , Test Acc With Reject: 0.0889\n",
      "Epoch: 077, Train Loss: 0.0055, Train Acc No Reject: 0.8426, Train Acc With Reject: 0.0222, Test Loss: 0.8095, Test Acc No Reject: 0.0222 , Test Acc With Reject: 0.0222\n",
      "Epoch: 078, Train Loss: 0.0055, Train Acc No Reject: 0.8389, Train Acc With Reject: 0.0778, Test Loss: 0.6615, Test Acc No Reject: 0.0778 , Test Acc With Reject: 0.0778\n",
      "Epoch: 079, Train Loss: 0.0054, Train Acc No Reject: 0.8475, Train Acc With Reject: 0.0556, Test Loss: 0.5875, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 080, Train Loss: 0.0055, Train Acc No Reject: 0.8525, Train Acc With Reject: 0.1056, Test Loss: 0.4700, Test Acc No Reject: 0.1056 , Test Acc With Reject: 0.1056\n",
      "Epoch: 081, Train Loss: 0.0053, Train Acc No Reject: 0.8321, Train Acc With Reject: 0.1111, Test Loss: 0.4548, Test Acc No Reject: 0.1111 , Test Acc With Reject: 0.1111\n",
      "Epoch: 082, Train Loss: 0.0054, Train Acc No Reject: 0.8309, Train Acc With Reject: 0.0556, Test Loss: 0.5072, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 083, Train Loss: 0.0054, Train Acc No Reject: 0.8426, Train Acc With Reject: 0.1833, Test Loss: 0.1094, Test Acc No Reject: 0.1833 , Test Acc With Reject: 0.1833\n",
      "Epoch: 084, Train Loss: 0.0056, Train Acc No Reject: 0.8444, Train Acc With Reject: 0.0278, Test Loss: 1.1147, Test Acc No Reject: 0.0278 , Test Acc With Reject: 0.0278\n",
      "Epoch: 085, Train Loss: 0.0056, Train Acc No Reject: 0.8377, Train Acc With Reject: 0.0889, Test Loss: 0.8975, Test Acc No Reject: 0.0889 , Test Acc With Reject: 0.0889\n",
      "Epoch: 086, Train Loss: 0.0053, Train Acc No Reject: 0.8444, Train Acc With Reject: 0.0556, Test Loss: 2.2198, Test Acc No Reject: 0.0556 , Test Acc With Reject: 0.0556\n",
      "Epoch: 087, Train Loss: 0.0053, Train Acc No Reject: 0.8463, Train Acc With Reject: 0.1111, Test Loss: 0.4140, Test Acc No Reject: 0.1111 , Test Acc With Reject: 0.1111\n",
      "Epoch: 088, Train Loss: 0.0053, Train Acc No Reject: 0.8568, Train Acc With Reject: 0.1333, Test Loss: 0.2643, Test Acc No Reject: 0.1333 , Test Acc With Reject: 0.1333\n",
      "Epoch: 089, Train Loss: 0.0053, Train Acc No Reject: 0.8352, Train Acc With Reject: 0.0667, Test Loss: 0.2890, Test Acc No Reject: 0.0667 , Test Acc With Reject: 0.0667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     35\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Adjust the learning rate\u001b[39;00m\n",
      "\u001b[1;32m     36\u001b[0m loss, train_acc_no_reject, train_acc_with_reject \u001b[38;5;241m=\u001b[39m train_epoch(model\u001b[38;5;241m=\u001b[39mmodel,\n",
      "\u001b[1;32m     37\u001b[0m                                 loader\u001b[38;5;241m=\u001b[39mtr_loader,\n",
      "\u001b[1;32m     38\u001b[0m                                 criterion\u001b[38;5;241m=\u001b[39mcriterion,\n",
      "\u001b[1;32m     39\u001b[0m                                 optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n",
      "\u001b[1;32m     40\u001b[0m                                 device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;32m---> 41\u001b[0m test_loss, test_acc_no_reject, test_acc_with_reject \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mte_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n",
      "\u001b[1;32m     44\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc No Reject: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc_no_reject\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc With Reject: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc_with_reject\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc No Reject: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc_no_reject\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , Test Acc With Reject: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc_with_reject\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;32m     45\u001b[0m )\n",
      "\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#print(optimizer.param_groups[0]['lr'])  # Print the current learning rate\u001b[39;00m\n",
      "\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Append results to lists\u001b[39;00m\n",
      "\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader, device)\u001b[0m\n",
      "\u001b[1;32m     91\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n",
      "\u001b[1;32m     92\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:  \u001b[38;5;66;03m# Iterate in batches over the training dataset.\u001b[39;00m\n",
      "\u001b[1;32m     94\u001b[0m     num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data\u001b[38;5;241m.\u001b[39mnum_graphs)\n",
      "\u001b[1;32m     95\u001b[0m     data\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n",
      "\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n",
      "\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n",
      "\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n",
      "\u001b[1;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n",
      "\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n",
      "\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n",
      "\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n",
      "\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n",
      "\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n",
      "\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[1;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/data/collate.py:109\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n",
      "\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n",
      "\u001b[0;32m--> 109\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n",
      "\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n",
      "\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/data/collate.py:171\u001b[0m, in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n",
      "\u001b[1;32m    169\u001b[0m     incs \u001b[38;5;241m=\u001b[39m get_incs(key, values, data_list, stores)\n",
      "\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(incs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m--> 171\u001b[0m         values \u001b[38;5;241m=\u001b[39m [\n",
      "\u001b[1;32m    172\u001b[0m             value \u001b[38;5;241m+\u001b[39m inc\u001b[38;5;241m.\u001b[39mto(value\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;32m    173\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value, inc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, incs)\n",
      "\u001b[1;32m    174\u001b[0m         ]\n",
      "\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    176\u001b[0m     incs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_geometric/data/collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    169\u001b[0m     incs \u001b[38;5;241m=\u001b[39m get_incs(key, values, data_list, stores)\n",
      "\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(incs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;32m    171\u001b[0m         values \u001b[38;5;241m=\u001b[39m [\n",
      "\u001b[0;32m--> 172\u001b[0m             \u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    173\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value, inc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, incs)\n",
      "\u001b[1;32m    174\u001b[0m         ]\n",
      "\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    176\u001b[0m     incs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# def train_epoch(*, model, loader, criterion, optimizer, device='cuda:0'):\n",
    "#     model.train()\n",
    "#     corrects_no_reject = 0\n",
    "#     corrects_with_reject = 0\n",
    "#     num_examples = 0\n",
    "#     total_loss = 0\n",
    "#     for data in loader:  # Iterate in batches over the training dataset.\n",
    "#         num_examples += int(data.num_graphs)\n",
    "#         data.to(device)\n",
    "#         out = model(torch.ones_like(data.x), data.edge_index,\n",
    "#                     data.batch)  # Perform a single forward pass.\n",
    "#         loss = criterion(out[:, :-2], data.y) #+ 1 * criterion(out[:, :-1], data.y)  # Compute the loss.\n",
    "#         #the prediction comes with rejection\n",
    "#         pred = out[:, :-2].argmax(dim=1)  # Use the class with highest probability.\n",
    "#         corrects_no_reject += int(\n",
    "#             (pred == data.y\n",
    "#              ).sum().detach().cpu())  # Check against ground-truth labels.\n",
    "#         pred = out[:, :-1].argmax(dim=1)  # Use the class with highest probability.\n",
    "#         corrects_with_reject += int(\n",
    "#             (pred == data.y\n",
    "#              ).sum().detach().cpu())  # Check against ground-truth labels.\n",
    "#         loss.backward()  # Derive gradients.\n",
    "#         optimizer.step()  # Update parameters based on gradients.\n",
    "#         optimizer.zero_grad()  # Clear gradients.\n",
    "#         total_loss += loss.detach().cpu().numpy()\n",
    "#     return total_loss/num_examples, corrects_no_reject / num_examples, corrects_with_reject / num_examples\n",
    "\n",
    "\n",
    "# def test(*, model, loader, device='cuda:0'):\n",
    "#     model.eval()\n",
    "#     corrects_no_reject = 0\n",
    "#     corrects_with_reject = 0\n",
    "#     total_loss = 0 \n",
    "#     num_examples = 0\n",
    "#     for data in loader:  # Iterate in batches over the training dataset.\n",
    "#         num_examples += int(data.num_graphs)\n",
    "#         data.to(device)\n",
    "#         out = model(torch.ones_like(data.x), data.edge_index,\n",
    "#                     data.batch)  # Perform a single forward pass.\n",
    "#         test_loss = criterion(out[:, :-2], data.y)\n",
    "#         pred = out[:, :-2].argmax(dim=1)  # Use the class with highest probability.\n",
    "#         corrects_no_reject += int(\n",
    "#             (pred == data.y\n",
    "#              ).sum().detach().cpu())  # Check against ground-truth labels.\n",
    "#         pred = out[:, :-1].argmax(dim=1)  # Use the class with highest probability.\n",
    "#         corrects_with_reject += int(\n",
    "#             (pred == data.y\n",
    "#              ).sum().detach().cpu())  # Check against ground-truth labels.\n",
    "#         total_loss += test_loss.detach().cpu().numpy()\n",
    "#     return total_loss/num_examples, corrects_no_reject / num_examples, corrects_with_reject / num_examples\n",
    "\n",
    "\n",
    "# # Create lists to store results\n",
    "# train_losses = []\n",
    "# train_acc_no_rejects = []\n",
    "# train_acc_with_rejects = []\n",
    "# test_losses = []\n",
    "# test_acc_no_rejects = []\n",
    "# test_acc_with_rejects = []\n",
    "\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# # Define your optimizer and learning rate parameters\n",
    "# initial_lr = 0.01  # Adjust the initial learning rate as needed\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)#, weight_decay=5e-4)\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, initial_lr, 0.0001, 5000)\n",
    "\n",
    "# # MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.1)\n",
    "\n",
    "# # warmup_epochs = 10  # Number of epochs for warm-up\n",
    "# total_epochs = 5000  # Total number of training epochs\n",
    "# # lr_lambda = lambda epoch: min(1.0, (epoch + 1) / warmup_epochs)  # Linear warm-up function\n",
    "\n",
    "# # Create the scheduler, monitoring validation loss\n",
    "# # lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "# # Define the learning rate scheduler\n",
    "# # scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(total_epochs):\n",
    "#     scheduler.step()  # Adjust the learning rate\n",
    "#     loss, train_acc_no_reject, train_acc_with_reject = train_epoch(model=model,\n",
    "#                                     loader=tr_loader,\n",
    "#                                     criterion=criterion,\n",
    "#                                     optimizer=optimizer,\n",
    "#                                     device=device)\n",
    "#     test_loss, test_acc_no_reject, test_acc_with_reject = test(model=model, loader=te_loader, device=device)\n",
    "    \n",
    "#     print(\n",
    "#         f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train Acc No Reject: {train_acc_no_reject:.4f}, Train Acc With Reject: {test_acc_with_reject:.4f}, Test Loss: {test_loss:.4f}, Test Acc No Reject: {test_acc_no_reject:.4f} , Test Acc With Reject: {test_acc_with_reject:.4f}'\n",
    "#     )\n",
    "#     #print(optimizer.param_groups[0]['lr'])  # Print the current learning rate\n",
    "#     # Append results to lists\n",
    "#     train_losses.append(loss)\n",
    "#     train_acc_no_rejects.append(train_acc_no_reject)\n",
    "#     train_acc_with_rejects.append(train_acc_with_reject)\n",
    "#     test_losses.append(test_loss)\n",
    "#     test_acc_no_rejects.append(test_acc_no_reject)\n",
    "#     test_acc_with_rejects.append(test_acc_with_reject)\n",
    "#     #lr_scheduler.step(test_loss)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
